{"cells":[{"cell_type":"code","source":["!pip install -q chromadb spacy sentence_transformers langchain_text_splitters"],"metadata":{"id":"nnOAV6Lp6uoJ","executionInfo":{"status":"ok","timestamp":1726605277464,"user_tz":-180,"elapsed":3963,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WYNCW3FZ68LP","executionInfo":{"status":"ok","timestamp":1726605285741,"user_tz":-180,"elapsed":8429,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}},"outputId":"8337d7f8-80fa-48e4-99e8-d21c68552928"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":31,"metadata":{"id":"2WeMvV1y2eHO","executionInfo":{"status":"ok","timestamp":1726605285742,"user_tz":-180,"elapsed":306,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"outputs":[],"source":["from chromadb.utils import embedding_functions\n","import chromadb\n","import numpy as np\n","import pandas as pd\n","import json\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"s05Qia2q2eHR","executionInfo":{"status":"ok","timestamp":1726605285742,"user_tz":-180,"elapsed":257,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"outputs":[],"source":["json_file ='/content/drive/MyDrive/AI_RAG/Data/AI.json'\n","\n","with open(json_file, 'r', encoding='utf-8') as f:\n","    data1 = json.load(f)"]},{"cell_type":"code","source":["json_file ='/content/drive/MyDrive/AI_RAG/Data/hands_on_machine_learning.json'\n","\n","with open(json_file, 'r', encoding='utf-8') as f:\n","    data2 = json.load(f)"],"metadata":{"id":"t6Z2Mf8pskpB","executionInfo":{"status":"ok","timestamp":1726605285743,"user_tz":-180,"elapsed":254,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["texts1 = [chapter['text'] for part in data1.values() for chapter in part['chapters']]"],"metadata":{"id":"tYpj28pJABYL","executionInfo":{"status":"ok","timestamp":1726605285743,"user_tz":-180,"elapsed":243,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["texts1[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161},"id":"VHPup4ibAGe9","executionInfo":{"status":"ok","timestamp":1726605285751,"user_tz":-180,"elapsed":247,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}},"outputId":"28e1cc16-bee6-46f2-c1f7-74bb735b9b6f"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1\\nINTRODUCTION\\nIn which we try to explain why we consider artiﬁcial intelligence to be a subject\\nmost worthy of study, and in which we try to decide what exactly it is, this being a\\ngood thing to decide before embarking.\\nWe call ourselves Homo sapiens—man the wise—because our intelligence is so important\\nINTELLIGENCE\\nto us. For thousands of years, we have tried to understand how we think; that is, how a mere\\nhandful of matter can perceive, understand, predict, and manipulate a world far larger and\\nmore complicated than itself. The ﬁeld of artiﬁcial intelligence, or AI, goes further still: it\\nARTIFICIAL\\nINTELLIGENCE\\nattempts not just to understand but also to build intelligent entities.\\nAI is one of the newest ﬁelds in science and engineering. Work started in earnest soon\\nafter World War II, and the name itself was coined in 1956. Along with molecular biology,\\nAI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other disciplines.\\nA student in physics might reasonably feel that all the good ideas have already been taken by\\nGalileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings for several\\nfull-time Einsteins and Edisons.\\nAI currently encompasses a huge variety of subﬁelds, ranging from the general (learning\\nand perception) to the speciﬁc, such as playing chess, proving mathematical theorems, writing\\npoetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any\\nintellectual task; it is truly a universal ﬁeld.\\n1.1\\nWHAT IS AI?\\nWe have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see\\neight deﬁnitions of AI, laid out along two dimensions. The deﬁnitions on top are concerned\\nwith thought processes and reasoning, whereas the ones on the bottom address behavior. The\\ndeﬁnitions on the left measure success in terms of ﬁdelity to human performance, whereas\\nthe ones on the right measure against an ideal performance measure, called rationality. A\\nRATIONALITY\\nsystem is rational if it does the “right thing,” given what it knows.\\nHistorically, all four approaches to AI have been followed, each by different people\\nwith different methods. A human-centered approach must be in part an empirical science, in-\\n1\\n2\\nChapter\\n1.\\nIntroduction\\nThinking Humanly\\nThinking Rationally\\n“The exciting new effort to make comput-\\ners think . . . machines with minds, in the\\nfull and literal sense.” (Haugeland, 1985)\\n“The study of mental faculties through the\\nuse of computational models.”\\n(Charniak and McDermott, 1985)\\n“[The automation of] activities that we\\nassociate with human thinking, activities\\nsuch as decision-making, problem solv-\\ning, learning . . .” (Bellman, 1978)\\n“The study of the computations that make\\nit possible to perceive, reason, and act.”\\n(Winston, 1992)\\nActing Humanly\\nActing Rationally\\n“The art of creating machines that per-\\nform functions that require intelligence\\nwhen performed by people.” (Kurzweil,\\n1990)\\n“Computational Intelligence is the study\\nof the design of intelligent agents.” (Poole\\net al., 1998)\\n“The study of how to make computers do\\nthings at which, at the moment, people are\\nbetter.” (Rich and Knight, 1991)\\n“AI . . . is concerned with intelligent be-\\nhavior in artifacts.” (Nilsson, 1998)\\nFigure 1.1\\nSome deﬁnitions of artiﬁcial intelligence, organized into four categories.\\nvolving observations and hypotheses about human behavior. A rationalist1 approach involves\\na combination of mathematics and engineering. The various group have both disparaged and\\nhelped each other. Let us look at the four approaches in more detail.\\n1.1.1\\nActing humanly: The Turing Test approach\\nThe Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory\\nTURING TEST\\noperational deﬁnition of intelligence. A computer passes the test if a human interrogator, after\\nposing some written questions, cannot tell whether the written responses come from a person\\nor from a computer. Chapter 26 discusses the details of the test and whether a computer would\\nreally be intelligent if it passed. For now, we note that programming a computer to pass a\\nrigorously applied test provides plenty to work on. The computer would need to possess the\\nfollowing capabilities:\\n• natural language processing to enable it to communicate successfully in English;\\nNATURAL LANGUAGE\\nPROCESSING\\n• knowledge representation to store what it knows or hears;\\nKNOWLEDGE\\nREPRESENTATION\\n• automated reasoning to use the stored information to answer questions and to draw\\nAUTOMATED\\nREASONING\\nnew conclusions;\\n• machine learning to adapt to new circumstances and to detect and extrapolate patterns.\\nMACHINE LEARNING\\n1 By distinguishing between human and rational behavior, we are not suggesting that humans are necessarily\\n“irrational” in the sense of “emotionally unstable” or “insane.” One merely need note that we are not perfect:\\nnot all chess players are grandmasters; and, unfortunately, not everyone gets an A on the exam. Some systematic\\nerrors in human reasoning are cataloged by Kahneman et al. (1982).\\nSection 1.1.\\nWhat Is AI?\\n3\\nTuring’s test deliberately avoided direct physical interaction between the interrogator and the\\ncomputer, because physical simulation of a person is unnecessary for intelligence. However,\\nthe so-called total Turing Test includes a video signal so that the interrogator can test the\\nTOTAL TURING TEST\\nsubject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical\\nobjects “through the hatch.” To pass the total Turing Test, the computer will need\\n• computer vision to perceive objects, and\\nCOMPUTER VISION\\n• robotics to manipulate objects and move about.\\nROBOTICS\\nThese six disciplines compose most of AI, and Turing deserves credit for designing a test\\nthat remains relevant 60 years later. Yet AI researchers have devoted little effort to passing\\nthe Turing Test, believing that it is more important to study the underlying principles of in-\\ntelligence than to duplicate an exemplar. The quest for “artiﬁcial ﬂight” succeeded when the\\nWright brothers and others stopped imitating birds and started using wind tunnels and learn-\\ning about aerodynamics. Aeronautical engineering texts do not deﬁne the goal of their ﬁeld\\nas making “machines that ﬂy so exactly like pigeons that they can fool even other pigeons.”\\n1.1.2\\nThinking humanly: The cognitive modeling approach\\nIf we are going to say that a given program thinks like a human, we must have some way of\\ndetermining how humans think. We need to get inside the actual workings of human minds.\\nThere are three ways to do this: through introspection—trying to catch our own thoughts as\\nthey go by; through psychological experiments—observing a person in action; and through\\nbrain imaging—observing the brain in action. Once we have a sufﬁciently precise theory of\\nthe mind, it becomes possible to express the theory as a computer program. If the program’s\\ninput–output behavior matches corresponding human behavior, that is evidence that some of\\nthe program’s mechanisms could also be operating in humans. For example, Allen Newell\\nand Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon,\\n1961), were not content merely to have their program solve problems correctly. They were\\nmore concerned with comparing the trace of its reasoning steps to traces of human subjects\\nsolving the same problems. The interdisciplinary ﬁeld of cognitive science brings together\\nCOGNITIVE SCIENCE\\ncomputer models from AI and experimental techniques from psychology to construct precise\\nand testable theories of the human mind.\\nCognitive science is a fascinating ﬁeld in itself, worthy of several textbooks and at least\\none encyclopedia (Wilson and Keil, 1999). We will occasionally comment on similarities or\\ndifferences between AI techniques and human cognition. Real cognitive science, however, is\\nnecessarily based on experimental investigation of actual humans or animals. We will leave\\nthat for other books, as we assume the reader has only a computer for experimentation.\\nIn the early days of AI there was often confusion between the approaches: an author\\nwould argue that an algorithm performs well on a task and that it is therefore a good model\\nof human performance, or vice versa. Modern authors separate the two kinds of claims;\\nthis distinction has allowed both AI and cognitive science to develop more rapidly. The two\\nﬁelds continue to fertilize each other, most notably in computer vision, which incorporates\\nneurophysiological evidence into computational models.\\n4\\nChapter\\n1.\\nIntroduction\\n1.1.3\\nThinking rationally: The “laws of thought” approach\\nThe Greek philosopher Aristotle was one of the ﬁrst to attempt to codify “right thinking,” that\\nis, irrefutable reasoning processes. His syllogisms provided patterns for argument structures\\nSYLLOGISM\\nthat always yielded correct conclusions when given correct premises—for example, “Socrates\\nis a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought were\\nsupposed to govern the operation of the mind; their study initiated the ﬁeld called logic.\\nLOGIC\\nLogicians in the 19th century developed a precise notation for statements about all kinds\\nof objects in the world and the relations among them. (Contrast this with ordinary arithmetic\\nnotation, which provides only for statements about numbers.) By 1965, programs existed\\nthat could, in principle, solve any solvable problem described in logical notation. (Although\\nif no solution exists, the program might loop forever.) The so-called logicist tradition within\\nLOGICIST\\nartiﬁcial intelligence hopes to build on such programs to create intelligent systems.\\nThere are two main obstacles to this approach. First, it is not easy to take informal\\nknowledge and state it in the formal terms required by logical notation, particularly when\\nthe knowledge is less than 100% certain. Second, there is a big difference between solving\\na problem “in principle” and solving it in practice. Even problems with just a few hundred\\nfacts can exhaust the computational resources of any computer unless it has some guidance\\nas to which reasoning steps to try ﬁrst. Although both of these obstacles apply to any attempt\\nto build computational reasoning systems, they appeared ﬁrst in the logicist tradition.\\n1.1.4\\nActing rationally: The rational agent approach\\nAn agent is just something that acts (agent comes from the Latin agere, to do). Of course,\\nAGENT\\nall computer programs do something, but computer agents are expected to do more: operate\\nautonomously, perceive their environment, persist over a prolonged time period, adapt to\\nchange, and create and pursue goals. A rational agent is one that acts so as to achieve the\\nRATIONAL AGENT\\nbest outcome or, when there is uncertainty, the best expected outcome.\\nIn the “laws of thought” approach to AI, the emphasis was on correct inferences. Mak-\\ning correct inferences is sometimes part of being a rational agent, because one way to act\\nrationally is to reason logically to the conclusion that a given action will achieve one’s goals\\nand then to act on that conclusion. On the other hand, correct inference is not all of ration-\\nality; in some situations, there is no provably correct thing to do, but something must still be\\ndone. There are also ways of acting rationally that cannot be said to involve inference. For\\nexample, recoiling from a hot stove is a reﬂex action that is usually more successful than a\\nslower action taken after careful deliberation.\\nAll the skills needed for the Turing Test also allow an agent to act rationally. Knowledge\\nrepresentation and reasoning enable agents to reach good decisions. We need to be able to\\ngenerate comprehensible sentences in natural language to get by in a complex society. We\\nneed learning not only for erudition, but also because it improves our ability to generate\\neffective behavior.\\nThe rational-agent approach has two advantages over the other approaches. First, it\\nis more general than the “laws of thought” approach because correct inference is just one\\nof several possible mechanisms for achieving rationality. Second, it is more amenable to\\nSection 1.2.\\nThe Foundations of Artiﬁcial Intelligence\\n5\\nscientiﬁc development than are approaches based on human behavior or human thought. The\\nstandard of rationality is mathematically well deﬁned and completely general, and can be\\n“unpacked” to generate agent designs that provably achieve it. Human behavior, on the other\\nhand, is well adapted for one speciﬁc environment and is deﬁned by, well, the sum total\\nof all the things that humans do. This book therefore concentrates on general principles\\nof rational agents and on components for constructing them. We will see that despite the\\napparent simplicity with which the problem can be stated, an enormous variety of issues\\ncome up when we try to solve it. Chapter 2 outlines some of these issues in more detail.\\nOne important point to keep in mind: We will see before too long that achieving perfect\\nrationality—always doing the right thing—is not feasible in complicated environments. The\\ncomputational demands are just too high. For most of the book, however, we will adopt the\\nworking hypothesis that perfect rationality is a good starting point for analysis. It simpliﬁes\\nthe problem and provides the appropriate setting for most of the foundational material in\\nthe ﬁeld. Chapters 5 and 17 deal explicitly with the issue of limited rationality—acting\\nLIMITED\\nRATIONALITY\\nappropriately when there is not enough time to do all the computations one might like.\\n1.2\\nTHE FOUNDATIONS OF ARTIFICIAL INTELLIGENCE\\nIn this section, we provide a brief history of the disciplines that contributed ideas, viewpoints,\\nand techniques to AI. Like any history, this one is forced to concentrate on a small number\\nof people, events, and ideas and to ignore others that also were important. We organize the\\nhistory around a series of questions. We certainly would not wish to give the impression that\\nthese questions are the only ones the disciplines address or that the disciplines have all been\\nworking toward AI as their ultimate fruition.\\n1.2.1\\nPhilosophy\\n• Can formal rules be used to draw valid conclusions?\\n• How does the mind arise from a physical brain?\\n• Where does knowledge come from?\\n• How does knowledge lead to action?\\nAristotle (384–322 B.C.), whose bust appears on the front cover of this book, was the ﬁrst\\nto formulate a precise set of laws governing the rational part of the mind. He developed an\\ninformal system of syllogisms for proper reasoning, which in principle allowed one to gener-\\nate conclusions mechanically, given initial premises. Much later, Ramon Lull (d. 1315) had\\nthe idea that useful reasoning could actually be carried out by a mechanical artifact. Thomas\\nHobbes (1588–1679) proposed that reasoning was like numerical computation, that “we add\\nand subtract in our silent thoughts.” The automation of computation itself was already well\\nunder way. Around 1500, Leonardo da Vinci (1452–1519) designed but did not build a me-\\nchanical calculator; recent reconstructions have shown the design to be functional. The ﬁrst\\nknown calculating machine was constructed around 1623 by the German scientist Wilhelm\\nSchickard (1592–1635), although the Pascaline, built in 1642 by Blaise Pascal (1623–1662),\\n6\\nChapter\\n1.\\nIntroduction\\nis more famous. Pascal wrote that “the arithmetical machine produces effects which appear\\nnearer to thought than all the actions of animals.” Gottfried Wilhelm Leibniz (1646–1716)\\nbuilt a mechanical device intended to carry out operations on concepts rather than numbers,\\nbut its scope was rather limited. Leibniz did surpass Pascal by building a calculator that\\ncould add, subtract, multiply, and take roots, whereas the Pascaline could only add and sub-\\ntract. Some speculated that machines might not just do calculations but actually be able to\\nthink and act on their own. In his 1651 book Leviathan, Thomas Hobbes suggested the idea\\nof an “artiﬁcial animal,” arguing “For what is the heart but a spring; and the nerves, but so\\nmany strings; and the joints, but so many wheels.”\\nIt’s one thing to say that the mind operates, at least in part, according to logical rules, and\\nto build physical systems that emulate some of those rules; it’s another to say that the mind\\nitself is such a physical system. Ren´e Descartes (1596–1650) gave the ﬁrst clear discussion\\nof the distinction between mind and matter and of the problems that arise. One problem with\\na purely physical conception of the mind is that it seems to leave little room for free will:\\nif the mind is governed entirely by physical laws, then it has no more free will than a rock\\n“deciding” to fall toward the center of the earth. Descartes was a strong advocate of the power\\nof reasoning in understanding the world, a philosophy now called rationalism, and one that\\nRATIONALISM\\ncounts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.\\nDUALISM\\nHe held that there is a part of the human mind (or soul or spirit) that is outside of nature,\\nexempt from physical laws. Animals, on the other hand, did not possess this dual quality;\\nthey could be treated as machines. An alternative to dualism is materialism, which holds\\nMATERIALISM\\nthat the brain’s operation according to the laws of physics constitutes the mind. Free will is\\nsimply the way that the perception of available choices appears to the choosing entity.\\nGiven a physical mind that manipulates knowledge, the next problem is to establish\\nthe source of knowledge. The empiricism movement, starting with Francis Bacon’s (1561–\\nEMPIRICISM\\n1626) Novum Organum,2 is characterized by a dictum of John Locke (1632–1704): “Nothing\\nis in the understanding, which was not ﬁrst in the senses.” David Hume’s (1711–1776) A\\nTreatise of Human Nature (Hume, 1739) proposed what is now known as the principle of\\ninduction: that general rules are acquired by exposure to repeated associations between their\\nINDUCTION\\nelements. Building on the work of Ludwig Wittgenstein (1889–1951) and Bertrand Russell\\n(1872–1970), the famous Vienna Circle, led by Rudolf Carnap (1891–1970), developed the\\ndoctrine of logical positivism. This doctrine holds that all knowledge can be characterized by\\nLOGICAL POSITIVISM\\nlogical theories connected, ultimately, to observation sentences that correspond to sensory\\nOBSERVATION\\nSENTENCES\\ninputs; thus logical positivism combines rationalism and empiricism.3 The conﬁrmation the-\\nory of Carnap and Carl Hempel (1905–1997) attempted to analyze the acquisition of knowl-\\nCONFIRMATION\\nTHEORY\\nedge from experience. Carnap’s book The Logical Structure of the World (1928) deﬁned an\\nexplicit computational procedure for extracting knowledge from elementary experiences. It\\nwas probably the ﬁrst theory of mind as a computational process.\\n2 The Novum Organum is an update of Aristotle’s Organon, or instrument of thought. Thus Aristotle can be\\nseen as both an empiricist and a rationalist.\\n3 In this picture, all meaningful statements can be veriﬁed or falsiﬁed either by experimentation or by analysis\\nof the meaning of the words. Because this rules out most of metaphysics, as was the intention, logical positivism\\nwas unpopular in some circles.\\nSection 1.2.\\nThe Foundations of Artiﬁcial Intelligence\\n7\\nThe ﬁnal element in the philosophical picture of the mind is the connection between\\nknowledge and action. This question is vital to AI because intelligence requires action as well\\nas reasoning. Moreover, only by understanding how actions are justiﬁed can we understand\\nhow to build an agent whose actions are justiﬁable (or rational). Aristotle argued (in De Motu\\nAnimalium) that actions are justiﬁed by a logical connection between goals and knowledge of\\nthe action’s outcome (the last part of this extract also appears on the front cover of this book,\\nin the original Greek):\\nBut how does it happen that thinking is sometimes accompanied by action and sometimes\\nnot, sometimes by motion, and sometimes not? It looks as if almost the same thing\\nhappens as in the case of reasoning and making inferences about unchanging objects. But\\nin that case the end is a speculative proposition . . . whereas here the conclusion which\\nresults from the two premises is an action. . . . I need covering; a cloak is a covering. I\\nneed a cloak. What I need, I have to make; I need a cloak. I have to make a cloak. And\\nthe conclusion, the “I have to make a cloak,” is an action.\\nIn the Nicomachean Ethics (Book III. 3, 1112b), Aristotle further elaborates on this topic,\\nsuggesting an algorithm:\\nWe deliberate not about ends, but about means. For a doctor does not deliberate whether\\nhe shall heal, nor an orator whether he shall persuade, . . . They assume the end and\\nconsider how and by what means it is attained, and if it seems easily and best produced\\nthereby; while if it is achieved by one means only they consider how it will be achieved\\nby this and by what means this will be achieved, till they come to the ﬁrst cause, . . . and\\nwhat is last in the order of analysis seems to be ﬁrst in the order of becoming. And if we\\ncome on an impossibility, we give up the search, e.g., if we need money and this cannot\\nbe got; but if a thing appears possible we try to do it.\\nAristotle’s algorithm was implemented 2300 years later by Newell and Simon in their GPS\\nprogram. We would now call it a regression planning system (see Chapter 10).\\nGoal-based analysis is useful, but does not say what to do when several actions will\\nachieve the goal or when no action will achieve it completely. Antoine Arnauld (1612–1694)\\ncorrectly described a quantitative formula for deciding what action to take in cases like this\\n(see Chapter 16). John Stuart Mill’s (1806–1873) book Utilitarianism (Mill, 1863) promoted\\nthe idea of rational decision criteria in all spheres of human activity. The more formal theory\\nof decisions is discussed in the following section.\\n1.2.2\\nMathematics\\n• What are the formal rules to draw valid conclusions?\\n• What can be computed?\\n• How do we reason with uncertain information?\\nPhilosophers staked out some of the fundamental ideas of AI, but the leap to a formal science\\nrequired a level of mathematical formalization in three fundamental areas: logic, computa-\\ntion, and probability.\\nThe idea of formal logic can be traced back to the philosophers of ancient Greece, but\\nits mathematical development really began with the work of George Boole (1815–1864), who\\n8\\nChapter\\n1.\\nIntroduction\\nworked out the details of propositional, or Boolean, logic (Boole, 1847). In 1879, Gottlob\\nFrege (1848–1925) extended Boole’s logic to include objects and relations, creating the ﬁrst-\\norder logic that is used today.4 Alfred Tarski (1902–1983) introduced a theory of reference\\nthat shows how to relate the objects in a logic to objects in the real world.\\nThe next step was to determine the limits of what could be done with logic and com-\\nputation. The ﬁrst nontrivial algorithm is thought to be Euclid’s algorithm for computing\\nALGORITHM\\ngreatest common divisors. The word algorithm (and the idea of studying them) comes from\\nal-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced\\nArabic numerals and algebra to Europe. Boole and others discussed algorithms for logical\\ndeduction, and, by the late 19th century, efforts were under way to formalize general mathe-\\nmatical reasoning as logical deduction. In 1930, Kurt G¨odel (1906–1978) showed that there\\nexists an effective procedure to prove any true statement in the ﬁrst-order logic of Frege and\\nRussell, but that ﬁrst-order logic could not capture the principle of mathematical induction\\nneeded to characterize the natural numbers. In 1931, G¨odel showed that limits on deduc-\\ntion do exist. His incompleteness theorem showed that in any formal theory as strong as\\nINCOMPLETENESS\\nTHEOREM\\nPeano arithmetic (the elementary theory of natural numbers), there are true statements that\\nare undecidable in the sense that they have no proof within the theory.\\nThis fundamental result can also be interpreted as showing that some functions on the\\nintegers cannot be represented by an algorithm—that is, they cannot be computed. This\\nmotivated Alan Turing (1912–1954) to try to characterize exactly which functions are com-\\nputable—capable of being computed. This notion is actually slightly problematic because\\nCOMPUTABLE\\nthe notion of a computation or effective procedure really cannot be given a formal deﬁnition.\\nHowever, the Church–Turing thesis, which states that the Turing machine (Turing, 1936) is\\ncapable of computing any computable function, is generally accepted as providing a sufﬁcient\\ndeﬁnition. Turing also showed that there were some functions that no Turing machine can\\ncompute. For example, no machine can tell in general whether a given program will return\\nan answer on a given input or run forever.\\nAlthough decidability and computability are important to an understanding of computa-\\ntion, the notion of tractability has had an even greater impact. Roughly speaking, a problem\\nTRACTABILITY\\nis called intractable if the time required to solve instances of the problem grows exponentially\\nwith the size of the instances. The distinction between polynomial and exponential growth\\nin complexity was ﬁrst emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965). It is\\nimportant because exponential growth means that even moderately large instances cannot be\\nsolved in any reasonable time. Therefore, one should strive to divide the overall problem of\\ngenerating intelligent behavior into tractable subproblems rather than intractable ones.\\nHow can one recognize an intractable problem? The theory of NP-completeness, pio-\\nNP-COMPLETENESS\\nneered by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp\\nshowed the existence of large classes of canonical combinatorial search and reasoning prob-\\nlems that are NP-complete. Any problem class to which the class of NP-complete problems\\ncan be reduced is likely to be intractable. (Although it has not been proved that NP-complete\\n4 Frege’s proposed notation for ﬁrst-order logic—an arcane combination of textual and geometric features—\\nnever became popular.\\nSection 1.2.\\nThe Foundations of Artiﬁcial Intelligence\\n9\\nproblems are necessarily intractable, most theoreticians believe it.) These results contrast\\nwith the optimism with which the popular press greeted the ﬁrst computers—“Electronic\\nSuper-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers,\\ncareful use of resources will characterize intelligent systems. Put crudely, the world is an\\nextremely large problem instance! Work in AI has helped explain why some instances of\\nNP-complete problems are hard, yet others are easy (Cheeseman et al., 1991).\\nBesides logic and computation, the third great contribution of mathematics to AI is the\\ntheory of probability. The Italian Gerolamo Cardano (1501–1576) ﬁrst framed the idea of\\nPROBABILITY\\nprobability, describing it in terms of the possible outcomes of gambling events. In 1654,\\nBlaise Pascal (1623–1662), in a letter to Pierre Fermat (1601–1665), showed how to pre-\\ndict the future of an unﬁnished gambling game and assign average payoffs to the gamblers.\\nProbability quickly became an invaluable part of all the quantitative sciences, helping to deal\\nwith uncertain measurements and incomplete theories. James Bernoulli (1654–1705), Pierre\\nLaplace (1749–1827), and others advanced the theory and introduced new statistical meth-\\nods. Thomas Bayes (1702–1761), who appears on the front cover of this book, proposed\\na rule for updating probabilities in the light of new evidence. Bayes’ rule underlies most\\nmodern approaches to uncertain reasoning in AI systems.\\n1.2.3\\nEconomics\\n• How should we make decisions so as to maximize payoff?\\n• How should we do this when others may not go along?\\n• How should we do this when the payoff may be far in the future?\\nThe science of economics got its start in 1776, when Scottish philosopher Adam Smith\\n(1723–1790) published An Inquiry into the Nature and Causes of the Wealth of Nations.\\nWhile the ancient Greeks and others had made contributions to economic thought, Smith was\\nthe ﬁrst to treat it as a science, using the idea that economies can be thought of as consist-\\ning of individual agents maximizing their own economic well-being. Most people think of\\neconomics as being about money, but economists will say that they are really studying how\\npeople make choices that lead to preferred outcomes. When McDonald’s offers a hamburger\\nfor a dollar, they are asserting that they would prefer the dollar and hoping that customers will\\nprefer the hamburger. The mathematical treatment of “preferred outcomes” or utility was\\nUTILITY\\nﬁrst formalized by L´eon Walras (pronounced “Valrasse”) (1834-1910) and was improved by\\nFrank Ramsey (1931) and later by John von Neumann and Oskar Morgenstern in their book\\nThe Theory of Games and Economic Behavior (1944).\\nDecision theory, which combines probability theory with utility theory, provides a for-\\nDECISION THEORY\\nmal and complete framework for decisions (economic or otherwise) made under uncertainty—\\nthat is, in cases where probabilistic descriptions appropriately capture the decision maker’s\\nenvironment. This is suitable for “large” economies where each agent need pay no attention\\nto the actions of other agents as individuals. For “small” economies, the situation is much\\nmore like a game: the actions of one player can signiﬁcantly affect the utility of another\\n(either positively or negatively). Von Neumann and Morgenstern’s development of game\\ntheory (see also Luce and Raiffa, 1957) included the surprising result that, for some games,\\nGAME THEORY\\n10\\nChapter\\n1.\\nIntroduction\\na rational agent should adopt policies that are (or least appear to be) randomized. Unlike de-\\ncision theory, game theory does not offer an unambiguous prescription for selecting actions.\\nFor the most part, economists did not address the third question listed above, namely,\\nhow to make rational decisions when payoffs from actions are not immediate but instead re-\\nsult from several actions taken in sequence. This topic was pursued in the ﬁeld of operations\\nresearch, which emerged in World War II from efforts in Britain to optimize radar installa-\\nOPERATIONS\\nRESEARCH\\ntions, and later found civilian applications in complex management decisions. The work of\\nRichard Bellman (1957) formalized a class of sequential decision problems called Markov\\ndecision processes, which we study in Chapters 17 and 21.\\nWork in economics and operations research has contributed much to our notion of ra-\\ntional agents, yet for many years AI research developed along entirely separate paths. One\\nreason was the apparent complexity of making rational decisions. The pioneering AI re-\\nsearcher Herbert Simon (1916–2001) won the Nobel Prize in economics in 1978 for his early\\nwork showing that models based on satisﬁcing—making decisions that are “good enough,”\\nSATISFICING\\nrather than laboriously calculating an optimal decision—gave a better description of actual\\nhuman behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in\\ndecision-theoretic techniques for agent systems (Wellman, 1995).\\n1.2.4\\nNeuroscience\\n• How do brains process information?\\nNeuroscience is the study of the nervous system, particularly the brain. Although the exact\\nNEUROSCIENCE\\nway in which the brain enables thought is one of the great mysteries of science, the fact that it\\ndoes enable thought has been appreciated for thousands of years because of the evidence that\\nstrong blows to the head can lead to mental incapacitation. It has also long been known that\\nhuman brains are somehow different; in about 335 B.C. Aristotle wrote, “Of all the animals,\\nman has the largest brain in proportion to his size.”5 Still, it was not until the middle of the\\n18th century that the brain was widely recognized as the seat of consciousness. Before then,\\ncandidate locations included the heart and the spleen.\\nPaul Broca’s (1824–1880) study of aphasia (speech deﬁcit) in brain-damaged patients\\nin 1861 demonstrated the existence of localized areas of the brain responsible for speciﬁc\\ncognitive functions. In particular, he showed that speech production was localized to the\\nportion of the left hemisphere now called Broca’s area.6 By that time, it was known that\\nthe brain consisted of nerve cells, or neurons, but it was not until 1873 that Camillo Golgi\\nNEURON\\n(1843–1926) developed a staining technique allowing the observation of individual neurons\\nin the brain (see Figure 1.2). This technique was used by Santiago Ramon y Cajal (1852–\\n1934) in his pioneering studies of the brain’s neuronal structures.7 Nicolas Rashevsky (1936,\\n1938) was the ﬁrst to apply mathematical models to the study of the nervous sytem.\\n5 Since then, it has been discovered that the tree shrew (Scandentia) has a higher ratio of brain to body mass.\\n6 Many cite Alexander Hood (1824) as a possible prior source.\\n7 Golgi persisted in his belief that the brain’s functions were carried out primarily in a continuous medium in\\nwhich neurons were embedded, whereas Cajal propounded the “neuronal doctrine.” The two shared the Nobel\\nprize in 1906 but gave mutually antagonistic acceptance speeches.\\nSection 1.2.\\nThe Foundations of Artiﬁcial Intelligence\\n11\\nAxon\\nCell body or Soma\\nNucleus\\nDendrite\\nSynapses\\nAxonal arborization\\nAxon from another cell\\nSynapse\\nFigure 1.2\\nThe parts of a nerve cell or neuron. Each neuron consists of a cell body,\\nor soma, that contains a cell nucleus. Branching out from the cell body are a number of\\nﬁbers called dendrites and a single long ﬁber called the axon. The axon stretches out for a\\nlong distance, much longer than the scale in this diagram indicates. Typically, an axon is\\n1 cm long (100 times the diameter of the cell body), but can reach up to 1 meter. A neuron\\nmakes connections with 10 to 100,000 other neurons at junctions called synapses. Signals are\\npropagated from neuron to neuron by a complicated electrochemical reaction. The signals\\ncontrol brain activity in the short term and also enable long-term changes in the connectivity\\nof neurons. These mechanisms are thought to form the basis for learning in the brain. Most\\ninformation processing goes on in the cerebral cortex, the outer layer of the brain. The basic\\norganizational unit appears to be a column of tissue about 0.5 mm in diameter, containing\\nabout 20,000 neurons and extending the full depth of the cortex about 4 mm in humans).\\nWe now have some data on the mapping between areas of the brain and the parts of the\\nbody that they control or from which they receive sensory input. Such mappings are able to\\nchange radically over the course of a few weeks, and some animals seem to have multiple\\nmaps. Moreover, we do not fully understand how other areas can take over functions when\\none area is damaged. There is almost no theory on how an individual memory is stored.\\nThe measurement of intact brain activity began in 1929 with the invention by Hans\\nBerger of the electroencephalograph (EEG). The recent development of functional magnetic\\nresonance imaging (fMRI) (Ogawa et al., 1990; Cabeza and Nyberg, 2001) is giving neu-\\nroscientists unprecedentedly detailed images of brain activity, enabling measurements that\\ncorrespond in interesting ways to ongoing cognitive processes. These are augmented by\\nadvances in single-cell recording of neuron activity. Individual neurons can be stimulated\\nelectrically, chemically, or even optically (Han and Boyden, 2007), allowing neuronal input–\\noutput relationships to be mapped. Despite these advances, we are still a long way from\\nunderstanding how cognitive processes actually work.\\nThe truly amazing conclusion is that a collection of simple cells can lead to thought,\\naction, and consciousness or, in the pithy words of John Searle (1992), brains cause minds.\\n12\\nChapter\\n1.\\nIntroduction\\nSupercomputer\\nPersonal Computer\\nHuman Brain\\nComputational units\\n104 CPUs, 1012 transistors 4 CPUs, 109 transistors 1011 neurons\\nStorage units\\n1014 bits RAM\\n1011 bits RAM\\n1011 neurons\\n1015 bits disk\\n1013 bits disk\\n1014 synapses\\nCycle time\\n10−9 sec\\n10−9 sec\\n10−3 sec\\nOperations/sec\\n1015\\n1010\\n1017\\nMemory updates/sec\\n1014\\n1010\\n1014\\nFigure 1.3\\nA crude comparison of the raw computational resources available to the IBM\\nBLUE GENE supercomputer, a typical personal computer of 2008, and the human brain. The\\nbrain’s numbers are essentially ﬁxed, whereas the supercomputer’s numbers have been in-\\ncreasing by a factor of 10 every 5 years or so, allowing it to achieve rough parity with the\\nbrain. The personal computer lags behind on all metrics except cycle time.\\nThe only real alternative theory is mysticism: that minds operate in some mystical realm that\\nis beyond physical science.\\nBrains and digital computers have somewhat different properties. Figure 1.3 shows that\\ncomputers have a cycle time that is a million times faster than a brain. The brain makes up\\nfor that with far more storage and interconnection than even a high-end personal computer,\\nalthough the largest supercomputers have a capacity that is similar to the brain’s. (It should\\nbe noted, however, that the brain does not seem to use all of its neurons simultaneously.)\\nFuturists make much of these numbers, pointing to an approaching singularity at which\\nSINGULARITY\\ncomputers reach a superhuman level of performance (Vinge, 1993; Kurzweil, 2005), but the\\nraw comparisons are not especially informative. Even with a computer of virtually unlimited\\ncapacity, we still would not know how to achieve the brain’s level of intelligence.\\n1.2.5\\nPsychology\\n• How do humans and animals think and act?\\nThe origins of scientiﬁc psychology are usually traced to the work of the German physi-\\ncist Hermann von Helmholtz (1821–1894) and his student Wilhelm Wundt (1832–1920).\\nHelmholtz applied the scientiﬁc method to the study of human vision, and his Handbook\\nof Physiological Optics is even now described as “the single most important treatise on the\\nphysics and physiology of human vision” (Nalwa, 1993, p.15). In 1879, Wundt opened the\\nﬁrst laboratory of experimental psychology, at the University of Leipzig. Wundt insisted\\non carefully controlled experiments in which his workers would perform a perceptual or as-\\nsociative task while introspecting on their thought processes. The careful controls went a\\nlong way toward making psychology a science, but the subjective nature of the data made\\nit unlikely that an experimenter would ever disconﬁrm his or her own theories. Biologists\\nstudying animal behavior, on the other hand, lacked introspective data and developed an ob-\\njective methodology, as described by H. S. Jennings (1906) in his inﬂuential work Behavior of\\nthe Lower Organisms. Applying this viewpoint to humans, the behaviorism movement, led\\nBEHAVIORISM\\nby John Watson (1878–1958), rejected any theory involving mental processes on the grounds\\nSection 1.2.\\nThe Foundations of Artiﬁcial Intelligence\\n13\\nthat introspection could not provide reliable evidence. Behaviorists insisted on studying only\\nobjective measures of the percepts (or stimulus) given to an animal and its resulting actions\\n(or response). Behaviorism discovered a lot about rats and pigeons but had less success at\\nunderstanding humans.\\nCognitive psychology, which views the brain as an information-processing device,\\nCOGNITIVE\\nPSYCHOLOGY\\ncan be traced back at least to the works of William James (1842–1910). Helmholtz also\\ninsisted that perception involved a form of unconscious logical inference. The cognitive\\nviewpoint was largely eclipsed by behaviorism in the United States, but at Cambridge’s Ap-\\nplied Psychology Unit, directed by Frederic Bartlett (1886–1969), cognitive modeling was\\nable to ﬂourish. The Nature of Explanation, by Bartlett’s student and successor Kenneth\\nCraik (1943), forcefully reestablished the legitimacy of such “mental” terms as beliefs and\\ngoals, arguing that they are just as scientiﬁc as, say, using pressure and temperature to talk\\nabout gases, despite their being made of molecules that have neither. Craik speciﬁed the\\nthree key steps of a knowledge-based agent: (1) the stimulus must be translated into an inter-\\nnal representation, (2) the representation is manipulated by cognitive processes to derive new\\ninternal representations, and (3) these are in turn retranslated back into action. He clearly\\nexplained why this was a good design for an agent:\\nIf the organism carries a “small-scale model” of external reality and of its own possible\\nactions within its head, it is able to try out various alternatives, conclude which is the best\\nof them, react to future situations before they arise, utilize the knowledge of past events\\nin dealing with the present and future, and in every way to react in a much fuller, safer,\\nand more competent manner to the emergencies which face it. (Craik, 1943)\\nAfter Craik’s death in a bicycle accident in 1945, his work was continued by Donald Broad-\\nbent, whose book Perception and Communication (1958) was one of the ﬁrst works to model\\npsychological phenomena as information processing. Meanwhile, in the United States, the\\ndevelopment of computer modeling led to the creation of the ﬁeld of cognitive science. The\\nﬁeld can be said to have started at a workshop in September 1956 at MIT. (We shall see that\\nthis is just two months after the conference at which AI itself was “born.”) At the workshop,\\nGeorge Miller presented The Magic Number Seven, Noam Chomsky presented Three Models\\nof Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine.\\nThese three inﬂuential papers showed how computer models could be used to address the\\npsychology of memory, language, and logical thinking, respectively. It is now a common\\n(although far from universal) view among psychologists that “a cognitive theory should be\\nlike a computer program” (Anderson, 1980); that is, it should describe a detailed information-\\nprocessing mechanism whereby some cognitive function might be implemented.\\n1.2.6\\nComputer engineering\\n• How can we build an efﬁcient computer?\\nFor artiﬁcial intelligence to succeed, we need two things: intelligence and an artifact. The\\ncomputer has been the artifact of choice. The modern digital electronic computer was in-\\nvented independently and almost simultaneously by scientists in three countries embattled in\\n14\\nChapter\\n1.\\nIntroduction\\nWorld War II. The ﬁrst operational computer was the electromechanical Heath Robinson,8\\nbuilt in 1940 by Alan Turing’s team for a single purpose: deciphering German messages. In\\n1943, the same group developed the Colossus, a powerful general-purpose machine based\\non vacuum tubes.9 The ﬁrst operational programmable computer was the Z-3, the inven-\\ntion of Konrad Zuse in Germany in 1941. Zuse also invented ﬂoating-point numbers and the\\nﬁrst high-level programming language, Plankalk¨ul. The ﬁrst electronic computer, the ABC,\\nwas assembled by John Atanasoff and his student Clifford Berry between 1940 and 1942\\nat Iowa State University. Atanasoff’s research received little support or recognition; it was\\nthe ENIAC, developed as part of a secret military project at the University of Pennsylvania\\nby a team including John Mauchly and John Eckert, that proved to be the most inﬂuential\\nforerunner of modern computers.\\nSince that time, each generation of computer hardware has brought an increase in speed\\nand capacity and a decrease in price. Performance doubled every 18 months or so until around\\n2005, when power dissipation problems led manufacturers to start multiplying the number of\\nCPU cores rather than the clock speed. Current expectations are that future increases in power\\nwill come from massive parallelism—a curious convergence with the properties of the brain.\\nOf course, there were calculating devices before the electronic computer. The earliest\\nautomated machines, dating from the 17th century, were discussed on page 6. The ﬁrst pro-\\ngrammable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752–1834),\\nthat used punched cards to store instructions for the pattern to be woven. In the mid-19th\\ncentury, Charles Babbage (1792–1871) designed two machines, neither of which he com-\\npleted. The Difference Engine was intended to compute mathematical tables for engineering\\nand scientiﬁc projects. It was ﬁnally built and shown to work in 1991 at the Science Museum\\nin London (Swade, 2000). Babbage’s Analytical Engine was far more ambitious: it included\\naddressable memory, stored programs, and conditional jumps and was the ﬁrst artifact capa-\\nble of universal computation. Babbage’s colleague Ada Lovelace, daughter of the poet Lord\\nByron, was perhaps the world’s ﬁrst programmer. (The programming language Ada is named\\nafter her.) She wrote programs for the unﬁnished Analytical Engine and even speculated that\\nthe machine could play chess or compose music.\\nAI also owes a debt to the software side of computer science, which has supplied the\\noperating systems, programming languages, and tools needed to write modern programs (and\\npapers about them). But this is one area where the debt has been repaid: work in AI has pio-\\nneered many ideas that have made their way back to mainstream computer science, including\\ntime sharing, interactive interpreters, personal computers with windows and mice, rapid de-\\nvelopment environments, the linked list data type, automatic storage management, and key\\nconcepts of symbolic, functional, declarative, and object-oriented programming.\\n8 Heath Robinson was a cartoonist famous for his depictions of whimsical and absurdly complicated contrap-\\ntions for everyday tasks such as buttering toast.\\n9 In the postwar period, Turing wanted to use these computers for AI research—for example, one of the ﬁrst\\nchess programs (Turing et al., 1953). His efforts were blocked by the British government.\\nSection 1.2.\\nThe Foundations of Artiﬁcial Intelligence\\n15\\n1.2.7\\nControl theory and cybernetics\\n• How can artifacts operate under their own control?\\nKtesibios of Alexandria (c. 250 B.C.) built the ﬁrst self-controlling machine: a water clock\\nwith a regulator that maintained a constant ﬂow rate. This invention changed the deﬁnition\\nof what an artifact could do. Previously, only living things could modify their behavior in\\nresponse to changes in the environment. Other examples of self-regulating feedback control\\nsystems include the steam engine governor, created by James Watt (1736–1819), and the\\nthermostat, invented by Cornelis Drebbel (1572–1633), who also invented the submarine.\\nThe mathematical theory of stable feedback systems was developed in the 19th century.\\nThe central ﬁgure in the creation of what is now called control theory was Norbert\\nCONTROL THEORY\\nWiener (1894–1964). Wiener was a brilliant mathematician who worked with Bertrand Rus-\\nsell, among others, before developing an interest in biological and mechanical control systems\\nand their connection to cognition. Like Craik (who also used control systems as psychological\\nmodels), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the\\nbehaviorist orthodoxy (Rosenblueth et al., 1943). They viewed purposive behavior as aris-\\ning from a regulatory mechanism trying to minimize “error”—the difference between current\\nstate and goal state. In the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts,\\nand John von Neumann, organized a series of inﬂuential conferences that explored the new\\nmathematical and computational models of cognition. Wiener’s book Cybernetics (1948) be-\\nCYBERNETICS\\ncame a bestseller and awoke the public to the possibility of artiﬁcially intelligent machines.\\nMeanwhile, in Britain, W. Ross Ashby (Ashby, 1940) pioneered similar ideas. Ashby, Alan\\nTuring, Grey Walter, and others formed the Ratio Club for “those who had Wiener’s ideas\\nbefore Wiener’s book appeared.” Ashby’s Design for a Brain (1948, 1952) elaborated on his\\nidea that intelligence could be created by the use of homeostatic devices containing appro-\\nHOMEOSTATIC\\npriate feedback loops to achieve stable adaptive behavior.\\nModern control theory, especially the branch known as stochastic optimal control, has\\nas its goal the design of systems that maximize an objective function over time. This roughly\\nOBJECTIVE\\nFUNCTION\\nmatches our view of AI: designing systems that behave optimally. Why, then, are AI and\\ncontrol theory two different ﬁelds, despite the close connections among their founders? The\\nanswer lies in the close coupling between the mathematical techniques that were familiar to\\nthe participants and the corresponding sets of problems that were encompassed in each world\\nview. Calculus and matrix algebra, the tools of control theory, lend themselves to systems that\\nare describable by ﬁxed sets of continuous variables, whereas AI was founded in part as a way\\nto escape from the these perceived limitations. The tools of logical inference and computation\\nallowed AI researchers to consider problems such as language, vision, and planning that fell\\ncompletely outside the control theorist’s purview.\\n1.2.8\\nLinguistics\\n• How does language relate to thought?\\nIn 1957, B. F. Skinner published Verbal Behavior. This was a comprehensive, detailed ac-\\ncount of the behaviorist approach to language learning, written by the foremost expert in\\n16\\nChapter\\n1.\\nIntroduction\\nthe ﬁeld. But curiously, a review of the book became as well known as the book itself, and\\nserved to almost kill off interest in behaviorism. The author of the review was the linguist\\nNoam Chomsky, who had just published a book on his own theory, Syntactic Structures.\\nChomsky pointed out that the behaviorist theory did not address the notion of creativity in\\nlanguage—it did not explain how a child could understand and make up sentences that he or\\nshe had never heard before. Chomsky’s theory—based on syntactic models going back to the\\nIndian linguist Panini (c. 350 B.C.)—could explain this, and unlike previous theories, it was\\nformal enough that it could in principle be programmed.\\nModern linguistics and AI, then, were “born” at about the same time, and grew up\\ntogether, intersecting in a hybrid ﬁeld called computational linguistics or natural language\\nCOMPUTATIONAL\\nLINGUISTICS\\nprocessing. The problem of understanding language soon turned out to be considerably more\\ncomplex than it seemed in 1957. Understanding language requires an understanding of the\\nsubject matter and context, not just an understanding of the structure of sentences. This might\\nseem obvious, but it was not widely appreciated until the 1960s. Much of the early work in\\nknowledge representation (the study of how to put knowledge into a form that a computer\\ncan reason with) was tied to language and informed by research in linguistics, which was\\nconnected in turn to decades of work on the philosophical analysis of language.\\n1.3\\nTHE HISTORY OF ARTIFICIAL INTELLIGENCE\\nWith the background material behind us, we are ready to cover the development of AI itself.\\n1.3.1\\nThe gestation of artiﬁcial intelligence (1943–1955)\\nThe ﬁrst work that is now generally recognized as AI was done by Warren McCulloch and\\nWalter Pitts (1943). They drew on three sources: knowledge of the basic physiology and\\nfunction of neurons in the brain; a formal analysis of propositional logic due to Russell and\\nWhitehead; and Turing’s theory of computation. They proposed a model of artiﬁcial neurons\\nin which each neuron is characterized as being “on” or “off,” with a switch to “on” occurring\\nin response to stimulation by a sufﬁcient number of neighboring neurons. The state of a\\nneuron was conceived of as “factually equivalent to a proposition which proposed its adequate\\nstimulus.” They showed, for example, that any computable function could be computed by\\nsome network of connected neurons, and that all the logical connectives (and, or, not, etc.)\\ncould be implemented by simple net structures. McCulloch and Pitts also suggested that\\nsuitably deﬁned networks could learn. Donald Hebb (1949) demonstrated a simple updating\\nrule for modifying the connection strengths between neurons. His rule, now called Hebbian\\nlearning, remains an inﬂuential model to this day.\\nHEBBIAN LEARNING\\nTwo undergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the\\nﬁrst neural network computer in 1950. The SNARC, as it was called, used 3000 vacuum\\ntubes and a surplus automatic pilot mechanism from a B-24 bomber to simulate a network of\\n40 neurons. Later, at Princeton, Minsky studied universal computation in neural networks.\\nHis Ph.D. committee was skeptical about whether this kind of work should be considered\\nSection 1.3.\\nThe History of Artiﬁcial Intelligence\\n17\\nmathematics, but von Neumann reportedly said, “If it isn’t now, it will be someday.” Minsky\\nwas later to prove inﬂuential theorems showing the limitations of neural network research.\\nThere were a number of early examples of work that can be characterized as AI, but\\nAlan Turing’s vision was perhaps the most inﬂuential. He gave lectures on the topic as early\\nas 1947 at the London Mathematical Society and articulated a persuasive agenda in his 1950\\narticle “Computing Machinery and Intelligence.” Therein, he introduced the Turing Test,\\nmachine learning, genetic algorithms, and reinforcement learning. He proposed the Child\\nProgramme idea, explaining “Instead of trying to produce a programme to simulate the adult\\nmind, why not rather try to produce one which simulated the child’s?”\\n1.3.2\\nThe birth of artiﬁcial intelligence (1956)\\nPrinceton was home to another inﬂuential ﬁgure in AI, John McCarthy. After receiving his\\nPhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan-\\nford and then to Dartmouth College, which was to become the ofﬁcial birthplace of the ﬁeld.\\nMcCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring\\ntogether U.S. researchers interested in automata theory, neural nets, and the study of intel-\\nligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The\\nproposal states:10\\nWe propose that a 2 month, 10 man study of artiﬁcial intelligence be carried\\nout during the summer of 1956 at Dartmouth College in Hanover, New Hamp-\\nshire. The study is to proceed on the basis of the conjecture that every aspect of\\nlearning or any other feature of intelligence can in principle be so precisely de-\\nscribed that a machine can be made to simulate it. An attempt will be made to ﬁnd\\nhow to make machines use language, form abstractions and concepts, solve kinds\\nof problems now reserved for humans, and improve themselves. We think that a\\nsigniﬁcant advance can be made in one or more of these problems if a carefully\\nselected group of scientists work on it together for a summer.\\nThere were 10 attendees in all, including Trenchard More from Princeton, Arthur Samuel\\nfrom IBM, and Ray Solomonoff and Oliver Selfridge from MIT.\\nTwo researchers from Carnegie Tech,11 Allen Newell and Herbert Simon, rather stole\\nthe show. Although the others had ideas and in some cases programs for particular appli-\\ncations such as checkers, Newell and Simon already had a reasoning program, the Logic\\nTheorist (LT), about which Simon claimed, “We have invented a computer program capable\\nof thinking non-numerically, and thereby solved the venerable mind–body problem.”12 Soon\\nafter the workshop, the program was able to prove most of the theorems in Chapter 2 of Rus-\\n10 This was the ﬁrst ofﬁcial usage of McCarthy’s term artiﬁcial intelligence. Perhaps “computational rationality”\\nwould have been more precise and less threatening, but “AI” has stuck. At the 50th anniversary of the Dartmouth\\nconference, McCarthy stated that he resisted the terms “computer” or “computational” in deference to Norbert\\nWeiner, who was promoting analog cybernetic devices rather than digital computers.\\n11 Now Carnegie Mellon University (CMU).\\n12 Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler and\\ntranslated it into machine code by hand. To avoid errors, they worked in parallel, calling out binary numbers to\\neach other as they wrote each instruction to make sure they agreed.\\n18\\nChapter\\n1.\\nIntroduction\\nsell and Whitehead’s Principia Mathematica. Russell was reportedly delighted when Simon\\nshowed him that the program had come up with a proof for one theorem that was shorter than\\nthe one in Principia. The editors of the Journal of Symbolic Logic were less impressed; they\\nrejected a paper coauthored by Newell, Simon, and Logic Theorist.\\nThe Dartmouth workshop did not lead to any new breakthroughs, but it did introduce\\nall the major ﬁgures to each other. For the next 20 years, the ﬁeld would be dominated by\\nthese people and their students and colleagues at MIT, CMU, Stanford, and IBM.\\nLooking at the proposal for the Dartmouth workshop (McCarthy et al., 1955), we can\\nsee why it was necessary for AI to become a separate ﬁeld. Why couldn’t all the work done\\nin AI have taken place under the name of control theory or operations research or decision\\ntheory, which, after all, have objectives similar to those of AI? Or why isn’t AI a branch\\nof mathematics? The ﬁrst answer is that AI from the start embraced the idea of duplicating\\nhuman faculties such as creativity, self-improvement, and language use. None of the other\\nﬁelds were addressing these issues. The second answer is methodology. AI is the only one\\nof these ﬁelds that is clearly a branch of computer science (although operations research does\\nshare an emphasis on computer simulations), and AI is the only ﬁeld to attempt to build\\nmachines that will function autonomously in complex, changing environments.\\n1.3.3\\nEarly enthusiasm, great expectations (1952–1969)\\nThe early years of AI were full of successes—in a limited way. Given the primitive comput-\\ners and programming tools of the time and the fact that only a few years earlier computers\\nwere seen as things that could do arithmetic and no more, it was astonishing whenever a com-\\nputer did anything remotely clever. The intellectual establishment, by and large, preferred to\\nbelieve that “a machine can never do X.” (See Chapter 26 for a long list of X’s gathered\\nby Turing.) AI researchers naturally responded by demonstrating one X after another. John\\nMcCarthy referred to this period as the “Look, Ma, no hands!” era.\\nNewell and Simon’s early success was followed up with the General Problem Solver,\\nor GPS. Unlike Logic Theorist, this program was designed from the start to imitate human\\nproblem-solving protocols. Within the limited class of puzzles it could handle, it turned out\\nthat the order in which the program considered subgoals and possible actions was similar to\\nthat in which humans approached the same problems. Thus, GPS was probably the ﬁrst pro-\\ngram to embody the “thinking humanly” approach. The success of GPS and subsequent pro-\\ngrams as models of cognition led Newell and Simon (1976) to formulate the famous physical\\nsymbol system hypothesis, which states that “a physical symbol system has the necessary and\\nPHYSICAL SYMBOL\\nSYSTEM\\nsufﬁcient means for general intelligent action.” What they meant is that any system (human\\nor machine) exhibiting intelligence must operate by manipulating data structures composed\\nof symbols. We will see later that this hypothesis has been challenged from many directions.\\nAt IBM, Nathaniel Rochester and his colleagues produced some of the ﬁrst AI pro-\\ngrams.\\nHerbert Gelernter (1959) constructed the Geometry Theorem Prover, which was\\nable to prove theorems that many students of mathematics would ﬁnd quite tricky. Starting\\nin 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually\\nlearned to play at a strong amateur level. Along the way, he disproved the idea that comput-\\nSection 1.3.\\nThe History of Artiﬁcial Intelligence\\n19\\ners can do only what they are told to: his program quickly learned to play a better game than\\nits creator. The program was demonstrated on television in February 1956, creating a strong\\nimpression. Like Turing, Samuel had trouble ﬁnding computer time. Working at night, he\\nused machines that were still on the testing ﬂoor at IBM’s manufacturing plant. Chapter 5\\ncovers game playing, and Chapter 21 explains the learning techniques used by Samuel.\\nJohn McCarthy moved from Dartmouth to MIT and there made three crucial contribu-\\ntions in one historic year: 1958. In MIT AI Lab Memo No. 1, McCarthy deﬁned the high-level\\nlanguage Lisp, which was to become the dominant AI programming language for the next 30\\nLISP\\nyears. With Lisp, McCarthy had the tool he needed, but access to scarce and expensive com-\\nputing resources was also a serious problem. In response, he and others at MIT invented time\\nsharing. Also in 1958, McCarthy published a paper entitled Programs with Common Sense,\\nin which he described the Advice Taker, a hypothetical program that can be seen as the ﬁrst\\ncomplete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthy’s\\nprogram was designed to use knowledge to search for solutions to problems. But unlike the\\nothers, it was to embody general knowledge of the world. For example, he showed how\\nsome simple axioms would enable the program to generate a plan to drive to the airport. The\\nprogram was also designed to accept new axioms in the normal course of operation, thereby\\nallowing it to achieve competence in new areas without being reprogrammed. The Advice\\nTaker thus embodied the central principles of knowledge representation and reasoning: that\\nit is useful to have a formal, explicit representation of the world and its workings and to be\\nable to manipulate that representation with deductive processes. It is remarkable how much\\nof the 1958 paper remains relevant today.\\n1958 also marked the year that Marvin Minsky moved to MIT. His initial collaboration\\nwith McCarthy did not last, however. McCarthy stressed representation and reasoning in for-\\nmal logic, whereas Minsky was more interested in getting programs to work and eventually\\ndeveloped an anti-logic outlook. In 1963, McCarthy started the AI lab at Stanford. His plan\\nto use logic to build the ultimate Advice Taker was advanced by J. A. Robinson’s discov-\\nery in 1965 of the resolution method (a complete theorem-proving algorithm for ﬁrst-order\\nlogic; see Chapter 9). Work at Stanford emphasized general-purpose methods for logical\\nreasoning. Applications of logic included Cordell Green’s question-answering and planning\\nsystems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute\\n(SRI). The latter project, discussed further in Chapter 25, was the ﬁrst to demonstrate the\\ncomplete integration of logical reasoning and physical activity.\\nMinsky supervised a series of students who chose limited problems that appeared to\\nrequire intelligence to solve. These limited domains became known as microworlds. James\\nMICROWORLD\\nSlagle’s SAINT program (1963) was able to solve closed-form calculus integration problems\\ntypical of ﬁrst-year college courses. Tom Evans’s ANALOGY program (1968) solved geo-\\nmetric analogy problems that appear in IQ tests. Daniel Bobrow’s STUDENT program (1967)\\nsolved algebra story problems, such as the following:\\nIf the number of customers Tom gets is twice the square of 20 percent of the number\\nof advertisements he runs, and the number of advertisements he runs is 45, what is the\\nnumber of customers Tom gets?\\n20\\nChapter\\n1.\\nIntroduction\\nRed\\nGreen\\nRed\\nGreen\\nGreen\\nBlue\\nBlue\\nRed\\nFigure 1.4\\nA scene from the blocks world. SHRDLU (Winograd, 1972) has just completed\\nthe command “Find a block which is taller than the one you are holding and put it in the box.”\\nThe most famous microworld was the blocks world, which consists of a set of solid blocks\\nplaced on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.4.\\nA typical task in this world is to rearrange the blocks in a certain way, using a robot hand\\nthat can pick up one block at a time. The blocks world was home to the vision project of\\nDavid Huffman (1971), the vision and constraint-propagation work of David Waltz (1975),\\nthe learning theory of Patrick Winston (1970), the natural-language-understanding program\\nof Terry Winograd (1972), and the planner of Scott Fahlman (1974).\\nEarly work building on the neural networks of McCulloch and Pitts also ﬂourished.\\nThe work of Winograd and Cowan (1963) showed how a large number of elements could\\ncollectively represent an individual concept, with a corresponding increase in robustness and\\nparallelism. Hebb’s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,\\n1960; Widrow, 1962), who called his networks adalines, and by Frank Rosenblatt (1962)\\nwith his perceptrons. The perceptron convergence theorem (Block et al., 1962) says that\\nthe learning algorithm can adjust the connection strengths of a perceptron to match any input\\ndata, provided such a match exists. These topics are covered in Chapter 20.\\n1.3.4\\nA dose of reality (1966–1973)\\nFrom the beginning, AI researchers were not shy about making predictions of their coming\\nsuccesses. The following statement by Herbert Simon in 1957 is often quoted:\\nIt is not my aim to surprise or shock you—but the simplest way I can summarize is to say\\nthat there are now in the world machines that think, that learn and that create. Moreover,\\nSection 1.3.\\nThe History of Artiﬁcial Intelligence\\n21\\ntheir ability to do these things is going to increase rapidly until—in a visible future—the\\nrange of problems they can handle will be coextensive with the range to which the human\\nmind has been applied.\\nTerms such as “visible future” can be interpreted in various ways, but Simon also made\\nmore concrete predictions: that within 10 years a computer would be chess champion, and\\na signiﬁcant mathematical theorem would be proved by machine. These predictions came\\ntrue (or approximately true) within 40 years rather than 10. Simon’s overconﬁdence was due\\nto the promising performance of early AI systems on simple examples. In almost all cases,\\nhowever, these early systems turned out to fail miserably when tried out on wider selections\\nof problems and on more difﬁcult problems.\\nThe ﬁrst kind of difﬁculty arose because most early programs knew nothing of their\\nsubject matter; they succeeded by means of simple syntactic manipulations. A typical story\\noccurred in early machine translation efforts, which were generously funded by the U.S. Na-\\ntional Research Council in an attempt to speed up the translation of Russian scientiﬁc papers\\nin the wake of the Sputnik launch in 1957. It was thought initially that simple syntactic trans-\\nformations based on the grammars of Russian and English, and word replacement from an\\nelectronic dictionary, would sufﬁce to preserve the exact meanings of sentences. The fact is\\nthat accurate translation requires background knowledge in order to resolve ambiguity and\\nestablish the content of the sentence. The famous retranslation of “the spirit is willing but\\nthe ﬂesh is weak” as “the vodka is good but the meat is rotten” illustrates the difﬁculties en-\\ncountered. In 1966, a report by an advisory committee found that “there has been no machine\\ntranslation of general scientiﬁc text, and none is in immediate prospect.” All U.S. government\\nfunding for academic translation projects was canceled. Today, machine translation is an im-\\nperfect but widely used tool for technical, commercial, government, and Internet documents.\\nThe second kind of difﬁculty was the intractability of many of the problems that AI was\\nattempting to solve. Most of the early AI programs solved problems by trying out different\\ncombinations of steps until the solution was found. This strategy worked initially because\\nmicroworlds contained very few objects and hence very few possible actions and very short\\nsolution sequences. Before the theory of computational complexity was developed, it was\\nwidely thought that “scaling up” to larger problems was simply a matter of faster hardware\\nand larger memories. The optimism that accompanied the development of resolution theorem\\nproving, for example, was soon dampened when researchers failed to prove theorems involv-\\ning more than a few dozen facts. The fact that a program can ﬁnd a solution in principle does\\nnot mean that the program contains any of the mechanisms needed to ﬁnd it in practice.\\nThe illusion of unlimited computational power was not conﬁned to problem-solving\\nprograms. Early experiments in machine evolution (now called genetic algorithms) (Fried-\\nMACHINE EVOLUTION\\nGENETIC\\nALGORITHM\\nberg, 1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by\\nmaking an appropriate series of small mutations to a machine-code program, one can gen-\\nerate a program with good performance for any particular task. The idea, then, was to try\\nrandom mutations with a selection process to preserve mutations that seemed useful. De-\\nspite thousands of hours of CPU time, almost no progress was demonstrated. Modern genetic\\nalgorithms use better representations and have shown more success.\\n22\\nChapter\\n1.\\nIntroduction\\nFailure to come to grips with the “combinatorial explosion” was one of the main criti-\\ncisms of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for the\\ndecision by the British government to end support for AI research in all but two universities.\\n(Oral tradition paints a somewhat different and more colorful picture, with political ambitions\\nand personal animosities whose description is beside the point.)\\nA third difﬁculty arose because of some fundamental limitations on the basic structures\\nbeing used to generate intelligent behavior. For example, Minsky and Papert’s book Percep-\\ntrons (1969) proved that, although perceptrons (a simple form of neural network) could be\\nshown to learn anything they were capable of representing, they could represent very little. In\\nparticular, a two-input perceptron (restricted to be simpler than the form Rosenblatt originally\\nstudied) could not be trained to recognize when its two inputs were different. Although their\\nresults did not apply to more complex, multilayer networks, research funding for neural-net\\nresearch soon dwindled to almost nothing. Ironically, the new back-propagation learning al-\\ngorithms for multilayer networks that were to cause an enormous resurgence in neural-net\\nresearch in the late 1980s were actually discovered ﬁrst in 1969 (Bryson and Ho, 1969).\\n1.3.5\\nKnowledge-based systems: The key to power? (1969–1979)\\nThe picture of problem solving that had arisen during the ﬁrst decade of AI research was of\\na general-purpose search mechanism trying to string together elementary reasoning steps to\\nﬁnd complete solutions. Such approaches have been called weak methods because, although\\nWEAK METHOD\\ngeneral, they do not scale up to large or difﬁcult problem instances. The alternative to weak\\nmethods is to use more powerful, domain-speciﬁc knowledge that allows larger reasoning\\nsteps and can more easily handle typically occurring cases in narrow areas of expertise. One\\nmight say that to solve a hard problem, you have to almost know the answer already.\\nThe DENDRAL program (Buchanan et al., 1969) was an early example of this approach.\\nIt was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon),\\nBruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel\\nlaureate geneticist) teamed up to solve the problem of inferring molecular structure from the\\ninformation provided by a mass spectrometer. The input to the program consists of the ele-\\nmentary formula of the molecule (e.g., C6H13NO2) and the mass spectrum giving the masses\\nof the various fragments of the molecule generated when it is bombarded by an electron beam.\\nFor example, the mass spectrum might contain a peak at m = 15, corresponding to the mass\\nof a methyl (CH3) fragment.\\nThe naive version of the program generated all possible structures consistent with the\\nformula, and then predicted what mass spectrum would be observed for each, comparing this\\nwith the actual spectrum. As one might expect, this is intractable for even moderate-sized\\nmolecules. The DENDRAL researchers consulted analytical chemists and found that they\\nworked by looking for well-known patterns of peaks in the spectrum that suggested common\\nsubstructures in the molecule. For example, the following rule is used to recognize a ketone\\n(C=O) subgroup (which weighs 28):\\nif there are two peaks at x1 and x2 such that\\n(a) x1 + x2 = M + 28 (M is the mass of the whole molecule);\\nSection 1.3.\\nThe History of Artiﬁcial Intelligence\\n23\\n(b) x1 −28 is a high peak;\\n(c) x2 −28 is a high peak;\\n(d) At least one of x1 and x2 is high.\\nthen there is a ketone subgroup\\nRecognizing that the molecule contains a particular substructure reduces the number of pos-\\nsible candidates enormously. DENDRAL was powerful because\\nAll the relevant theoretical knowledge to solve these problems has been mapped over from\\nits general form in the [spectrum prediction component] (“ﬁrst principles”) to efﬁcient\\nspecial forms (“cookbook recipes”). (Feigenbaum et al., 1971)\\nThe signiﬁcance of DENDRAL was that it was the ﬁrst successful knowledge-intensive sys-\\ntem: its expertise derived from large numbers of special-purpose rules. Later systems also\\nincorporated the main theme of McCarthy’s Advice Taker approach—the clean separation of\\nthe knowledge (in the form of rules) from the reasoning component.\\nWith this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Pro-\\ngramming Project (HPP) to investigate the extent to which the new methodology of expert\\nsystems could be applied to other areas of human expertise. The next major effort was in\\nEXPERT SYSTEMS\\nthe area of medical diagnosis. Feigenbaum, Buchanan, and Dr. Edward Shortliffe developed\\nMYCIN to diagnose blood infections. With about 450 rules, MYCIN was able to perform\\nas well as some experts, and considerably better than junior doctors. It also contained two\\nmajor differences from DENDRAL. First, unlike the DENDRAL rules, no general theoretical\\nmodel existed from which the MYCIN rules could be deduced. They had to be acquired from\\nextensive interviewing of experts, who in turn acquired them from textbooks, other experts,\\nand direct experience of cases. Second, the rules had to reﬂect the uncertainty associated with\\nmedical knowledge. MYCIN incorporated a calculus of uncertainty called certainty factors\\nCERTAINTY FACTOR\\n(see Chapter 14), which seemed (at the time) to ﬁt well with how doctors assessed the impact\\nof evidence on the diagnosis.\\nThe importance of domain knowledge was also apparent in the area of understanding\\nnatural language. Although Winograd’s SHRDLU system for understanding natural language\\nhad engendered a good deal of excitement, its dependence on syntactic analysis caused some\\nof the same problems as occurred in the early machine translation work. It was able to\\novercome ambiguity and understand pronoun references, but this was mainly because it was\\ndesigned speciﬁcally for one area—the blocks world. Several researchers, including Eugene\\nCharniak, a fellow graduate student of Winograd’s at MIT, suggested that robust language\\nunderstanding would require general knowledge about the world and a general method for\\nusing that knowledge.\\nAt Yale, linguist-turned-AI-researcher Roger Schank emphasized this point, claiming,\\n“There is no such thing as syntax,” which upset a lot of linguists but did serve to start a useful\\ndiscussion. Schank and his students built a series of programs (Schank and Abelson, 1977;\\nWilensky, 1978; Schank and Riesbeck, 1981; Dyer, 1983) that all had the task of under-\\nstanding natural language. The emphasis, however, was less on language per se and more on\\nthe problems of representing and reasoning with the knowledge required for language under-\\nstanding. The problems included representing stereotypical situations (Cullingford, 1981),\\n24\\nChapter\\n1.\\nIntroduction\\ndescribing human memory organization (Rieger, 1976; Kolodner, 1983), and understanding\\nplans and goals (Wilensky, 1983).\\nThe widespread growth of applications to real-world problems caused a concurrent in-\\ncrease in the demands for workable knowledge representation schemes. A large number\\nof different representation and reasoning languages were developed. Some were based on\\nlogic—for example, the Prolog language became popular in Europe, and the PLANNER fam-\\nily in the United States. Others, following Minsky’s idea of frames (1975), adopted a more\\nFRAMES\\nstructured approach, assembling facts about particular object and event types and arranging\\nthe types into a large taxonomic hierarchy analogous to a biological taxonomy.\\n1.3.6\\nAI becomes an industry (1980–present)\\nThe ﬁrst successful commercial expert system, R1, began operation at the Digital Equipment\\nCorporation (McDermott, 1982). The program helped conﬁgure orders for new computer\\nsystems; by 1986, it was saving the company an estimated $40 million a year. By 1988,\\nDEC’s AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in\\nuse and 500 in development, saving an estimated $10 million a year. Nearly every major U.S.\\ncorporation had its own AI group and was either using or investigating expert systems.\\nIn 1981, the Japanese announced the “Fifth Generation” project, a 10-year plan to build\\nintelligent computers running Prolog. In response, the United States formed the Microelec-\\ntronics and Computer Technology Corporation (MCC) as a research consortium designed to\\nassure national competitiveness. In both cases, AI was part of a broad effort, including chip\\ndesign and human-interface research. In Britain, the Alvey report reinstated the funding that\\nwas cut by the Lighthill report.13 In all three countries, however, the projects never met their\\nambitious goals.\\nOverall, the AI industry boomed from a few million dollars in 1980 to billions of dollars\\nin 1988, including hundreds of companies building expert systems, vision systems, robots,\\nand software and hardware specialized for these purposes. Soon after that came a period\\ncalled the “AI Winter,” in which many companies fell by the wayside as they failed to deliver\\non extravagant promises.\\n1.3.7\\nThe return of neural networks (1986–present)\\nIn the mid-1980s at least four different groups reinvented the back-propagation learning\\nBACK-PROPAGATION\\nalgorithm ﬁrst found in 1969 by Bryson and Ho. The algorithm was applied to many learn-\\ning problems in computer science and psychology, and the widespread dissemination of the\\nresults in the collection Parallel Distributed Processing (Rumelhart and McClelland, 1986)\\ncaused great excitement.\\nThese so-called connectionist models of intelligent systems were seen by some as di-\\nCONNECTIONIST\\nrect competitors both to the symbolic models promoted by Newell and Simon and to the\\nlogicist approach of McCarthy and others (Smolensky, 1988). It might seem obvious that\\nat some level humans manipulate symbols—in fact, Terrence Deacon’s book The Symbolic\\n13 To save embarrassment, a new ﬁeld called IKBS (Intelligent Knowledge-Based Systems) was invented because\\nArtiﬁcial Intelligence had been ofﬁcially canceled.\\nSection 1.3.\\nThe History of Artiﬁcial Intelligence\\n25\\nSpecies (1997) suggests that this is the deﬁning characteristic of humans—but the most ar-\\ndent connectionists questioned whether symbol manipulation had any real explanatory role in\\ndetailed models of cognition. This question remains unanswered, but the current view is that\\nconnectionist and symbolic approaches are complementary, not competing. As occurred with\\nthe separation of AI and cognitive science, modern neural network research has bifurcated\\ninto two ﬁelds, one concerned with creating effective network architectures and algorithms\\nand understanding their mathematical properties, the other concerned with careful modeling\\nof the empirical properties of actual neurons and ensembles of neurons.\\n1.3.8\\nAI adopts the scientiﬁc method (1987–present)\\nRecent years have seen a revolution in both the content and the methodology of work in\\nartiﬁcial intelligence.14 It is now more common to build on existing theories than to propose\\nbrand-new ones, to base claims on rigorous theorems or hard experimental evidence rather\\nthan on intuition, and to show relevance to real-world applications rather than toy examples.\\nAI was founded in part as a rebellion against the limitations of existing ﬁelds like control\\ntheory and statistics, but now it is embracing those ﬁelds. As David McAllester (1998) put it:\\nIn the early period of AI it seemed plausible that new forms of symbolic computation,\\ne.g., frames and semantic networks, made much of classical theory obsolete. This led to\\na form of isolationism in which AI became largely separated from the rest of computer\\nscience. This isolationism is currently being abandoned. There is a recognition that\\nmachine learning should not be isolated from information theory, that uncertain reasoning\\nshould not be isolated from stochastic modeling, that search should not be isolated from\\nclassical optimization and control, and that automated reasoning should not be isolated\\nfrom formal methods and static analysis.\\nIn terms of methodology, AI has ﬁnally come ﬁrmly under the scientiﬁc method. To be ac-\\ncepted, hypotheses must be subjected to rigorous empirical experiments, and the results must\\nbe analyzed statistically for their importance (Cohen, 1995). It is now possible to replicate\\nexperiments by using shared repositories of test data and code.\\nThe ﬁeld of speech recognition illustrates the pattern. In the 1970s, a wide variety of\\ndifferent architectures and approaches were tried. Many of these were rather ad hoc and\\nfragile, and were demonstrated on only a few specially selected examples. In recent years,\\napproaches based on hidden Markov models (HMMs) have come to dominate the area. Two\\nHIDDEN MARKOV\\nMODELS\\naspects of HMMs are relevant. First, they are based on a rigorous mathematical theory. This\\nhas allowed speech researchers to build on several decades of mathematical results developed\\nin other ﬁelds. Second, they are generated by a process of training on a large corpus of\\nreal speech data. This ensures that the performance is robust, and in rigorous blind tests the\\nHMMs have been improving their scores steadily. Speech technology and the related ﬁeld of\\nhandwritten character recognition are already making the transition to widespread industrial\\n14 Some have characterized this change as a victory of the neats—those who think that AI theories should be\\ngrounded in mathematical rigor—over the scrufﬁes—those who would rather try out lots of ideas, write some\\nprograms, and then assess what seems to be working. Both approaches are important. A shift toward neatness\\nimplies that the ﬁeld has reached a level of stability and maturity. Whether that stability will be disrupted by a\\nnew scruffy idea is another question.\\n26\\nChapter\\n1.\\nIntroduction\\nand consumer applications. Note that there is no scientiﬁc claim that humans use HMMs to\\nrecognize speech; rather, HMMs provide a mathematical framework for understanding the\\nproblem and support the engineering claim that they work well in practice.\\nMachine translation follows the same course as speech recognition. In the 1950s there\\nwas initial enthusiasm for an approach based on sequences of words, with models learned\\naccording to the principles of information theory. That approach fell out of favor in the\\n1960s, but returned in the late 1990s and now dominates the ﬁeld.\\nNeural networks also ﬁt this trend. Much of the work on neural nets in the 1980s was\\ndone in an attempt to scope out what could be done and to learn how neural nets differ from\\n“traditional” techniques. Using improved methodology and theoretical frameworks, the ﬁeld\\narrived at an understanding in which neural nets can now be compared with corresponding\\ntechniques from statistics, pattern recognition, and machine learning, and the most promising\\ntechnique can be applied to each application. As a result of these developments, so-called\\ndata mining technology has spawned a vigorous new industry.\\nDATA MINING\\nJudea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new accep-\\ntance of probability and decision theory in AI, following a resurgence of interest epitomized\\nby Peter Cheeseman’s (1985) article “In Defense of Probability.” The Bayesian network\\nBAYESIAN NETWORK\\nformalism was invented to allow efﬁcient representation of, and rigorous reasoning with,\\nuncertain knowledge. This approach largely overcomes many problems of the probabilistic\\nreasoning systems of the 1960s and 1970s; it now dominates AI research on uncertain reason-\\ning and expert systems. The approach allows for learning from experience, and it combines\\nthe best of classical AI and neural nets. Work by Judea Pearl (1982a) and by Eric Horvitz and\\nDavid Heckerman (Horvitz and Heckerman, 1986; Horvitz et al., 1986) promoted the idea of\\nnormative expert systems: ones that act rationally according to the laws of decision theory\\nand do not try to imitate the thought steps of human experts. The WindowsTM operating sys-\\ntem includes several normative diagnostic expert systems for correcting problems. Chapters\\n13 to 16 cover this area.\\nSimilar gentle revolutions have occurred in robotics, computer vision, and knowledge\\nrepresentation. A better understanding of the problems and their complexity properties, com-\\nbined with increased mathematical sophistication, has led to workable research agendas and\\nrobust methods. Although increased formalization and specialization led ﬁelds such as vision\\nand robotics to become somewhat isolated from “mainstream” AI in the 1990s, this trend has\\nreversed in recent years as tools from machine learning in particular have proved effective for\\nmany problems. The process of reintegration is already yielding signiﬁcant beneﬁts\\n1.3.9\\nThe emergence of intelligent agents (1995–present)\\nPerhaps encouraged by the progress in solving the subproblems of AI, researchers have also\\nstarted to look at the “whole agent” problem again. The work of Allen Newell, John Laird,\\nand Paul Rosenbloom on SOAR (Newell, 1990; Laird et al., 1987) is the best-known example\\nof a complete agent architecture. One of the most important environments for intelligent\\nagents is the Internet. AI systems have become so common in Web-based applications that\\nthe “-bot” sufﬁx has entered everyday language. Moreover, AI technologies underlie many\\nSection 1.3.\\nThe History of Artiﬁcial Intelligence\\n27\\nInternet tools, such as search engines, recommender systems, and Web site aggregators.\\nOne consequence of trying to build complete agents is the realization that the previously\\nisolated subﬁelds of AI might need to be reorganized somewhat when their results are to be\\ntied together. In particular, it is now widely appreciated that sensory systems (vision, sonar,\\nspeech recognition, etc.) cannot deliver perfectly reliable information about the environment.\\nHence, reasoning and planning systems must be able to handle uncertainty. A second major\\nconsequence of the agent perspective is that AI has been drawn into much closer contact\\nwith other ﬁelds, such as control theory and economics, that also deal with agents. Recent\\nprogress in the control of robotic cars has derived from a mixture of approaches ranging from\\nbetter sensors, control-theoretic integration of sensing, localization and mapping, as well as\\na degree of high-level planning.\\nDespite these successes, some inﬂuential founders of AI, including John McCarthy\\n(2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and\\nWinston, 2009), have expressed discontent with the progress of AI. They think that AI should\\nput less emphasis on creating ever-improved versions of applications that are good at a spe-\\nciﬁc task, such as driving a car, playing chess, or recognizing speech. Instead, they believe\\nAI should return to its roots of striving for, in Simon’s words, “machines that think, that learn\\nand that create.” They call the effort human-level AI or HLAI; their ﬁrst symposium was in\\nHUMAN-LEVEL AI\\n2004 (Minsky et al., 2004). The effort will require very large knowledge bases; Hendler et al.\\n(1995) discuss where these knowledge bases might come from.\\nA related idea is the subﬁeld of Artiﬁcial General Intelligence or AGI (Goertzel and\\nARTIFICIAL GENERAL\\nINTELLIGENCE\\nPennachin, 2007), which held its ﬁrst conference and organized the Journal of Artiﬁcial Gen-\\neral Intelligence in 2008. AGI looks for a universal algorithm for learning and acting in\\nany environment, and has its roots in the work of Ray Solomonoff (1964), one of the atten-\\ndees of the original 1956 Dartmouth conference. Guaranteeing that what we create is really\\nFriendly AI is also a concern (Yudkowsky, 2008; Omohundro, 2008), one we will return to\\nFRIENDLY AI\\nin Chapter 26.\\n1.3.10\\nThe availability of very large data sets (2001–present)\\nThroughout the 60-year history of computer science, the emphasis has been on the algorithm\\nas the main subject of study. But some recent work in AI suggests that for many problems, it\\nmakes more sense to worry about the data and be less picky about what algorithm to apply.\\nThis is true because of the increasing availability of very large data sources: for example,\\ntrillions of words of English and billions of images from the Web (Kilgarriff and Grefenstette,\\n2006); or billions of base pairs of genomic sequences (Collins et al., 2003).\\nOne inﬂuential paper in this line was Yarowsky’s (1995) work on word-sense disam-\\nbiguation: given the use of the word “plant” in a sentence, does that refer to ﬂora or factory?\\nPrevious approaches to the problem had relied on human-labeled examples combined with\\nmachine learning algorithms. Yarowsky showed that the task can be done, with accuracy\\nabove 96%, with no labeled examples at all. Instead, given a very large corpus of unanno-\\ntated text and just the dictionary deﬁnitions of the two senses—“works, industrial plant” and\\n“ﬂora, plant life”—one can label examples in the corpus, and from there bootstrap to learn\\n28\\nChapter\\n1.\\nIntroduction\\nnew patterns that help label new examples. Banko and Brill (2001) show that techniques\\nlike this perform even better as the amount of available text goes from a million words to a\\nbillion and that the increase in performance from using more data exceeds any difference in\\nalgorithm choice; a mediocre algorithm with 100 million words of unlabeled training data\\noutperforms the best known algorithm with 1 million words.\\nAs another example, Hays and Efros (2007) discuss the problem of ﬁlling in holes in a\\nphotograph. Suppose you use Photoshop to mask out an ex-friend from a group photo, but\\nnow you need to ﬁll in the masked area with something that matches the background. Hays\\nand Efros deﬁned an algorithm that searches through a collection of photos to ﬁnd something\\nthat will match. They found the performance of their algorithm was poor when they used\\na collection of only ten thousand photos, but crossed a threshold into excellent performance\\nwhen they grew the collection to two million photos.\\nWork like this suggests that the “knowledge bottleneck” in AI—the problem of how to\\nexpress all the knowledge that a system needs—may be solved in many applications by learn-\\ning methods rather than hand-coded knowledge engineering, provided the learning algorithms\\nhave enough data to go on (Halevy et al., 2009). Reporters have noticed the surge of new ap-\\nplications and have written that “AI Winter” may be yielding to a new Spring (Havenstein,\\n2005). As Kurzweil (2005) writes, “today, many thousands of AI applications are deeply\\nembedded in the infrastructure of every industry.”\\n1.4\\nTHE STATE OF THE ART\\nWhat can AI do today? A concise answer is difﬁcult because there are so many activities in\\nso many subﬁelds. Here we sample a few applications; others appear throughout the book.\\nRobotic vehicles: A driverless robotic car named STANLEY sped through the rough\\nterrain of the Mojave dessert at 22 mph, ﬁnishing the 132-mile course ﬁrst to win the 2005\\nDARPA Grand Challenge. STANLEY is a Volkswagen Touareg outﬁtted with cameras, radar,\\nand laser rangeﬁnders to sense the environment and onboard software to command the steer-\\ning, braking, and acceleration (Thrun, 2006). The following year CMU’s BOSS won the Ur-\\nban Challenge, safely driving in trafﬁc through the streets of a closed Air Force base, obeying\\ntrafﬁc rules and avoiding pedestrians and other vehicles.\\nSpeech recognition: A traveler calling United Airlines to book a ﬂight can have the en-\\ntire conversation guided by an automated speech recognition and dialog management system.\\nAutonomous planning and scheduling: A hundred million miles from Earth, NASA’s\\nRemote Agent program became the ﬁrst on-board autonomous planning program to control\\nthe scheduling of operations for a spacecraft (Jonsson et al., 2000). REMOTE AGENT gen-\\nerated plans from high-level goals speciﬁed from the ground and monitored the execution of\\nthose plans—detecting, diagnosing, and recovering from problems as they occurred. Succes-\\nsor program MAPGEN (Al-Chang et al., 2004) plans the daily operations for NASA’s Mars\\nExploration Rovers, and MEXAR2 (Cesta et al., 2007) did mission planning—both logistics\\nand science planning—for the European Space Agency’s Mars Express mission in 2008.\\nSection 1.5.\\nSummary\\n29\\nGame playing: IBM’s DEEP BLUE became the ﬁrst computer program to defeat the\\nworld champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 in\\nan exhibition match (Goodman and Keene, 1997). Kasparov said that he felt a “new kind of\\nintelligence” across the board from him. Newsweek magazine described the match as “The\\nbrain’s last stand.” The value of IBM’s stock increased by $18 billion. Human champions\\nstudied Kasparov’s loss and were able to draw a few matches in subsequent years, but the\\nmost recent human-computer matches have been won convincingly by the computer.\\nSpam ﬁghting: Each day, learning algorithms classify over a billion messages as spam,\\nsaving the recipient from having to waste time deleting what, for many users, could comprise\\n80% or 90% of all messages, if not classiﬁed away by algorithms. Because the spammers are\\ncontinually updating their tactics, it is difﬁcult for a static programmed approach to keep up,\\nand learning algorithms work best (Sahami et al., 1998; Goodman and Heckerman, 2004).\\nLogistics planning: During the Persian Gulf crisis of 1991, U.S. forces deployed a\\nDynamic Analysis and Replanning Tool, DART (Cross and Walker, 1994), to do automated\\nlogistics planning and scheduling for transportation. This involved up to 50,000 vehicles,\\ncargo, and people at a time, and had to account for starting points, destinations, routes, and\\nconﬂict resolution among all parameters. The AI planning techniques generated in hours\\na plan that would have taken weeks with older methods. The Defense Advanced Research\\nProject Agency (DARPA) stated that this single application more than paid back DARPA’s\\n30-year investment in AI.\\nRobotics: The iRobot Corporation has sold over two million Roomba robotic vacuum\\ncleaners for home use. The company also deploys the more rugged PackBot to Iraq and\\nAfghanistan, where it is used to handle hazardous materials, clear explosives, and identify\\nthe location of snipers.\\nMachine Translation: A computer program automatically translates from Arabic to\\nEnglish, allowing an English speaker to see the headline “Ardogan Conﬁrms That Turkey\\nWould Not Accept Any Pressure, Urging Them to Recognize Cyprus.” The program uses a\\nstatistical model built from examples of Arabic-to-English translations and from examples of\\nEnglish text totaling two trillion words (Brants et al., 2007). None of the computer scientists\\non the team speak Arabic, but they do understand statistics and machine learning algorithms.\\nThese are just a few examples of artiﬁcial intelligence systems that exist today. Not\\nmagic or science ﬁction—but rather science, engineering, and mathematics, to which this\\nbook provides an introduction.\\n1.5\\nSUMMARY\\nThis chapter deﬁnes AI and establishes the cultural background against which it has devel-\\noped. Some of the important points are as follows:\\n• Different people approach AI with different goals in mind. Two important questions to\\nask are: Are you concerned with thinking or behavior? Do you want to model humans\\nor work from an ideal standard?\\n30\\nChapter\\n1.\\nIntroduction\\n• In this book, we adopt the view that intelligence is concerned mainly with rational\\naction. Ideally, an intelligent agent takes the best possible action in a situation. We\\nstudy the problem of building agents that are intelligent in this sense.\\n• Philosophers (going back to 400 B.C.) made AI conceivable by considering the ideas\\nthat the mind is in some ways like a machine, that it operates on knowledge encoded in\\nsome internal language, and that thought can be used to choose what actions to take.\\n• Mathematicians provided the tools to manipulate statements of logical certainty as well\\nas uncertain, probabilistic statements. They also set the groundwork for understanding\\ncomputation and reasoning about algorithms.\\n• Economists formalized the problem of making decisions that maximize the expected\\noutcome to the decision maker.\\n• Neuroscientists discovered some facts about how the brain works and the ways in which\\nit is similar to and different from computers.\\n• Psychologists adopted the idea that humans and animals can be considered information-\\nprocessing machines. Linguists showed that language use ﬁts into this model.\\n• Computer engineers provided the ever-more-powerful machines that make AI applica-\\ntions possible.\\n• Control theory deals with designing devices that act optimally on the basis of feedback\\nfrom the environment. Initially, the mathematical tools of control theory were quite\\ndifferent from AI, but the ﬁelds are coming closer together.\\n• The history of AI has had cycles of success, misplaced optimism, and resulting cutbacks\\nin enthusiasm and funding. There have also been cycles of introducing new creative\\napproaches and systematically reﬁning the best ones.\\n• AI has advanced more rapidly in the past decade because of greater use of the scientiﬁc\\nmethod in experimenting with and comparing approaches.\\n• Recent progress in understanding the theoretical basis for intelligence has gone hand in\\nhand with improvements in the capabilities of real systems. The subﬁelds of AI have\\nbecome more integrated, and AI has found common ground with other disciplines.\\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\\nThe methodological status of artiﬁcial intelligence is investigated in The Sciences of the Artiﬁ-\\ncial, by Herb Simon (1981), which discusses research areas concerned with complex artifacts.\\nIt explains how AI can be viewed as both science and mathematics. Cohen (1995) gives an\\noverview of experimental methodology within AI.\\nThe Turing Test (Turing, 1950) is discussed by Shieber (1994), who severely criticizes\\nthe usefulness of its instantiation in the Loebner Prize competition, and by Ford and Hayes\\n(1995), who argue that the test itself is not helpful for AI. Bringsjord (2008) gives advice for\\na Turing Test judge. Shieber (2004) and Epstein et al. (2008) collect a number of essays on\\nthe Turing Test. Artiﬁcial Intelligence: The Very Idea, by John Haugeland (1985), gives a\\nExercises\\n31\\nreadable account of the philosophical and practical problems of AI. Signiﬁcant early papers\\nin AI are anthologized in the collections by Webber and Nilsson (1981) and by Luger (1995).\\nThe Encyclopedia of AI (Shapiro, 1992) contains survey articles on almost every topic in\\nAI, as does Wikipedia. These articles usually provide a good entry point into the research\\nliterature on each topic. An insightful and comprehensive history of AI is given by Nils\\nNillson (2009), one of the early pioneers of the ﬁeld.\\nThe most recent work appears in the proceedings of the major AI conferences: the bi-\\nennial International Joint Conference on AI (IJCAI), the annual European Conference on AI\\n(ECAI), and the National Conference on AI, more often known as AAAI, after its sponsoring\\norganization. The major journals for general AI are Artiﬁcial Intelligence, Computational\\nIntelligence, the IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE In-\\ntelligent Systems, and the electronic Journal of Artiﬁcial Intelligence Research. There are also\\nmany conferences and journals devoted to speciﬁc areas, which we cover in the appropriate\\nchapters. The main professional societies for AI are the American Association for Artiﬁcial\\nIntelligence (AAAI), the ACM Special Interest Group in Artiﬁcial Intelligence (SIGART),\\nand the Society for Artiﬁcial Intelligence and Simulation of Behaviour (AISB). AAAI’s AI\\nMagazine contains many topical and tutorial articles, and its Web site, aaai.org, contains\\nnews, tutorials, and background information.\\nEXERCISES\\nThese exercises are intended to stimulate discussion, and some might be set as term projects.\\nAlternatively, preliminary attempts can be made now, and these attempts can be reviewed\\nafter the completion of the book.\\n1.1\\nDeﬁne in your own words: (a) intelligence, (b) artiﬁcial intelligence, (c) agent, (d)\\nrationality, (e) logical reasoning.\\n1.2\\nRead Turing’s original paper on AI (Turing, 1950). In the paper, he discusses several\\nobjections to his proposed enterprise and his test for intelligence. Which objections still carry\\nweight? Are his refutations valid? Can you think of new objections arising from develop-\\nments since he wrote the paper? In the paper, he predicts that, by the year 2000, a computer\\nwill have a 30% chance of passing a ﬁve-minute Turing Test with an unskilled interrogator.\\nWhat chance do you think a computer would have today? In another 50 years?\\n1.3\\nAre reﬂex actions (such as ﬂinching from a hot stove) rational? Are they intelligent?\\n1.4\\nSuppose we extend Evans’s ANALOGY program so that it can score 200 on a standard\\nIQ test. Would we then have a program more intelligent than a human? Explain.\\n1.5\\nThe neural structure of the sea slug Aplysia has been widely studied (ﬁrst by Nobel\\nLaureate Eric Kandel) because it has only about 20,000 neurons, most of them large and\\neasily manipulated. Assuming that the cycle time for an Aplysia neuron is roughly the same\\nas for a human neuron, how does the computational power, in terms of memory updates per\\nsecond, compare with the high-end computer described in Figure 1.3?\\n32\\nChapter\\n1.\\nIntroduction\\n1.6\\nHow could introspection—reporting on one’s inner thoughts—be inaccurate? Could I\\nbe wrong about what I’m thinking? Discuss.\\n1.7\\nTo what extent are the following computer systems instances of artiﬁcial intelligence:\\n• Supermarket bar code scanners.\\n• Web search engines.\\n• Voice-activated telephone menus.\\n• Internet routing algorithms that respond dynamically to the state of the network.\\n1.8\\nMany of the computational models of cognitive activities that have been proposed in-\\nvolve quite complex mathematical operations, such as convolving an image with a Gaussian\\nor ﬁnding a minimum of the entropy function. Most humans (and certainly all animals) never\\nlearn this kind of mathematics at all, almost no one learns it before college, and almost no\\none can compute the convolution of a function with a Gaussian in their head. What sense\\ndoes it make to say that the “vision system” is doing this kind of mathematics, whereas the\\nactual person has no idea how to do it?\\n1.9\\nWhy would evolution tend to result in systems that act rationally? What goals are such\\nsystems designed to achieve?\\n1.10\\nIs AI a science, or is it engineering? Or neither or both? Explain.\\n1.11\\n“Surely computers cannot be intelligent—they can do only what their programmers\\ntell them.” Is the latter statement true, and does it imply the former?\\n1.12\\n“Surely animals cannot be intelligent—they can do only what their genes tell them.”\\nIs the latter statement true, and does it imply the former?\\n1.13\\n“Surely animals, humans, and computers cannot be intelligent—they can do only what\\ntheir constituent atoms are told to do by the laws of physics.” Is the latter statement true, and\\ndoes it imply the former?\\n1.14\\nExamine the AI literature to discover whether the following tasks can currently be\\nsolved by computers:\\na. Playing a decent game of table tennis (Ping-Pong).\\nb. Driving in the center of Cairo, Egypt.\\nc. Driving in Victorville, California.\\nd. Buying a week’s worth of groceries at the market.\\ne. Buying a week’s worth of groceries on the Web.\\nf. Playing a decent game of bridge at a competitive level.\\ng. Discovering and proving new mathematical theorems.\\nh. Writing an intentionally funny story.\\ni. Giving competent legal advice in a specialized area of law.\\nj. Translating spoken English into spoken Swedish in real time.\\nk. Performing a complex surgical operation.\\nExercises\\n33\\nFor the currently infeasible tasks, try to ﬁnd out what the difﬁculties are and predict when, if\\never, they will be overcome.\\n1.15\\nVarious subﬁelds of AI have held contests by deﬁning a standard task and inviting re-\\nsearchers to do their best. Examples include the DARPA Grand Challenge for robotic cars,\\nThe International Planning Competition, the Robocup robotic soccer league, the TREC infor-\\nmation retrieval event, and contests in machine translation, speech recognition. Investigate\\nﬁve of these contests, and describe the progress made over the years. To what degree have the\\ncontests advanced toe state of the art in AI? Do what degree do they hurt the ﬁeld by drawing\\nenergy away from new ideas?\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["texts2 = [chapter['text'] for part in data2 for chapter in part['chapters']]"],"metadata":{"id":"RHQZPOORs0hX","executionInfo":{"status":"ok","timestamp":1726605285751,"user_tz":-180,"elapsed":211,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["texts2[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161},"id":"WYZY-1_rtiAX","executionInfo":{"status":"ok","timestamp":1726605285752,"user_tz":-180,"elapsed":205,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}},"outputId":"4b205b0e-4443-4f42-fbe9-0c377483d939"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Chapter 1. The Machine\\nLearning Landscape\\nWhen most people hear “Machine Learning,” they picture a robot: a\\ndependable butler or a deadly Terminator, depending on who you ask. But\\nMachine Learning is not just a futuristic fantasy; it’s already here. In fact,\\nit has been around for decades in some specialized applications, such as\\nOptical Character Recognition (OCR). But the first ML application that\\nreally became mainstream, improving the lives of hundreds of millions of\\npeople, took over the world back in the 1990s: the spam filter. It’s not\\nexactly a self-aware Skynet, but it does technically qualify as Machine\\nLearning (it has actually learned so well that you seldom need to flag an\\nemail as spam anymore). It was followed by hundreds of ML applications\\nthat now quietly power hundreds of products and features that you use\\nregularly, from better recommendations to voice search.\\nWhere does Machine Learning start and where does it end? What exactly\\ndoes it mean for a machine to learn something? If I download a copy of\\nWikipedia, has my computer really learned something? Is it suddenly\\nsmarter? In this chapter we will start by clarifying what Machine Learning\\nis and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we\\nwill take a look at the map and learn about the main regions and the most\\nnotable landmarks: supervised versus unsupervised learning, online versus\\nbatch learning, instance-based versus model-based learning. Then we will\\nlook at the workflow of a typical ML project, discuss the main challenges\\nyou may face, and cover how to evaluate and fine-tune a Machine\\nLearning system.\\nThis chapter introduces a lot of fundamental concepts (and jargon) that\\nevery data scientist should know by heart. It will be a high-level overview\\n(it’s the only chapter without much code), all rather simple, but you should\\nmake sure everything is crystal clear to you before continuing on to the\\nrest of the book. So grab a coffee and let’s get started!\\nTIP\\nIf you already know all the Machine Learning basics, you may want to skip directly\\nto Chapter 2. If you are not sure, try to answer all the questions listed at the end of\\nthe chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so\\nthey can learn from data.\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability\\nto learn without being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to\\nsome task T and some performance measure P, if its performance on T,\\nas measured by P, improves with experience E.\\n—Tom Mitchell, 1997\\nYour spam filter is a Machine Learning program that, given examples of\\nspam emails (e.g., flagged by users) and examples of regular (nonspam,\\nalso called “ham”) emails, can learn to flag spam. The examples that the\\nsystem uses to learn are called the training set. Each training example is\\ncalled a training instance (or sample). In this case, the task T is to flag\\nspam for new emails, the experience E is the training data, and the\\nperformance measure P needs to be defined; for example, you can use the\\nratio of correctly classified emails. This particular performance measure is\\ncalled accuracy, and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more\\ndata, but it is not suddenly better at any task. Thus, downloading a copy of\\nWikipedia is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional\\nprogramming techniques (Figure 1-1):\\n1. First you would consider what spam typically looks like. You\\nmight notice that some words or phrases (such as “4U,” “credit\\ncard,” “free,” and “amazing”) tend to come up a lot in the subject\\nline. Perhaps you would also notice a few other patterns in the\\nsender’s name, the email’s body, and other parts of the email.\\n2. You would write a detection algorithm for each of the patterns\\nthat you noticed, and your program would flag emails as spam if a\\nnumber of these patterns were detected.\\n3. You would test your program and repeat steps 1 and 2 until it was\\ngood enough to launch.\\nFigure 1-1. The traditional approach\\nSince the problem is difficult, your program will likely become a long list\\nof complex rules—pretty hard to maintain.\\nIn contrast, a spam filter based on Machine Learning techniques\\nautomatically learns which words and phrases are good predictors of spam\\nby detecting unusually frequent patterns of words in the spam examples\\ncompared to the ham examples (Figure 1-2). The program is much shorter,\\neasier to maintain, and most likely more accurate.\\nWhat if spammers notice that all their emails containing “4U” are\\nblocked? They might start writing “For U” instead. A spam filter using\\ntraditional programming techniques would need to be updated to flag “For\\nU” emails. If spammers keep working around your spam filter, you will\\nneed to keep writing new rules forever.\\nIn contrast, a spam filter based on Machine Learning techniques\\nautomatically notices that “For U” has become unusually frequent in spam\\nflagged by users, and it starts flagging them without your intervention\\n(Figure 1-3).\\nFigure 1-2. The Machine Learning approach\\nFigure 1-3. Automatically adapting to change\\nAnother area where Machine Learning shines is for problems that either\\nare too complex for traditional approaches or have no known algorithm.\\nFor example, consider speech recognition. Say you want to start simple\\nand write a program capable of distinguishing the words “one” and “two.”\\nYou might notice that the word “two” starts with a high-pitch sound (“T”),\\nso you could hardcode an algorithm that measures high-pitch sound\\nintensity and use that to distinguish ones and twos —but obviously this\\ntechnique will not scale to thousands of words spoken by millions of very\\ndifferent people in noisy environments and in dozens of languages. The\\nbest solution (at least today) is to write an algorithm that learns by itself,\\ngiven many example recordings for each word.\\nFinally, Machine Learning can help humans learn (Figure 1-4). ML\\nalgorithms can be inspected to see what they have learned (although for\\nsome algorithms this can be tricky). For instance, once a spam filter has\\nbeen trained on enough spam, it can easily be inspected to reveal the list of\\nwords and combinations of words that it believes are the best predictors of\\nspam. Sometimes this will reveal unsuspected correlations or new trends,\\nand thereby lead to a better understanding of the problem. Applying ML\\ntechniques to dig into large amounts of data can help discover patterns that\\nwere not immediately apparent. This is called data mining.\\nFigure 1-4. Machine Learning can help humans learn\\nTo summarize, Machine Learning is great for:\\nProblems for which existing solutions require a lot of fine-tuning\\nor long lists of rules: one Machine Learning algorithm can often\\nsimplify code and perform better than the traditional approach.\\nComplex problems for which using a traditional approach yields\\nno good solution: the best Machine Learning techniques can\\nperhaps find a solution.\\nFluctuating environments: a Machine Learning system can adapt\\nto new data.\\nGetting insights about complex problems and large amounts of\\ndata.\\nExamples of Applications\\nLet’s look at some concrete examples of Machine Learning tasks, along\\nwith the techniques that can tackle them:\\nAnalyzing images of products on a production line to automatically\\nclassify them\\nThis is image classification, typically performed using convolutional\\nneural networks (CNNs; see Chapter 14).\\nDetecting tumors in brain scans\\nThis is semantic segmentation, where each pixel in the image is\\nclassified (as we want to determine the exact location and shape of\\ntumors), typically using CNNs as well.\\nAutomatically classifying news articles\\nThis is natural language processing (NLP), and more specifically text\\nclassification, which can be tackled using recurrent neural networks\\n(RNNs), CNNs, or Transformers (see Chapter 16).\\nAutomatically flagging offensive comments on discussion forums\\nThis is also text classification, using the same NLP tools.\\nSummarizing long documents automatically\\nThis is a branch of NLP called text summarization, again using the\\nsame tools.\\nCreating a chatbot or a personal assistant\\nThis involves many NLP components, including natural language\\nunderstanding (NLU) and question-answering modules.\\nForecasting your company’s revenue next year, based on many\\nperformance metrics\\nThis is a regression task (i.e., predicting values) that may be tackled\\nusing any regression model, such as a Linear Regression or Polynomial\\nRegression model (see Chapter 4), a regression SVM (see Chapter 5), a\\nregression Random Forest (see Chapter 7), or an artificial neural\\nnetwork (see Chapter 10). If you want to take into account sequences\\nof past performance metrics, you may want to use RNNs, CNNs, or\\nTransformers (see Chapters 15 and 16).\\nMaking your app react to voice commands\\nThis is speech recognition, which requires processing audio samples:\\nsince they are long and complex sequences, they are typically\\nprocessed using RNNs, CNNs, or Transformers (see Chapters 15 and\\n16).\\nDetecting credit card fraud\\nThis is anomaly detection (see Chapter 9).\\nSegmenting clients based on their purchases so that you can design a\\ndifferent marketing strategy for each segment\\nThis is clustering (see Chapter 9).\\nRepresenting a complex, high-dimensional dataset in a clear and insightful\\ndiagram\\nThis is data visualization, often involving dimensionality reduction\\ntechniques (see Chapter 8).\\nRecommending a product that a client may be interested in, based on past\\npurchases\\nThis is a recommender system. One approach is to feed past purchases\\n(and other information about the client) to an artificial neural network\\n(see Chapter 10), and get it to output the most likely next purchase.\\nThis neural net would typically be trained on past sequences of\\npurchases across all clients.\\nBuilding an intelligent bot for a game\\nThis is often tackled using Reinforcement Learning (RL; see\\nChapter 18), which is a branch of Machine Learning that trains agents\\n(such as bots) to pick the actions that will maximize their rewards over\\ntime (e.g., a bot may get a reward every time the player loses some life\\npoints), within a given environment (such as the game). The famous\\nAlphaGo program that beat the world champion at the game of Go was\\nbuilt using RL.\\nThis list could go on and on, but hopefully it gives you a sense of the\\nincredible breadth and complexity of the tasks that Machine Learning can\\ntackle, and the types of techniques that you would use for each task.\\nTypes of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is\\nuseful to classify them in broad categories, based on the following criteria:\\nWhether or not they are trained with human supervision\\n(supervised, unsupervised, semisupervised, and Reinforcement\\nLearning)\\nWhether or not they can learn incrementally on the fly (online\\nversus batch learning)\\nWhether they work by simply comparing new data points to\\nknown data points, or instead by detecting patterns in the training\\ndata and building a predictive model, much like scientists do\\n(instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you\\nlike. For example, a state-of-the-art spam filter may learn on the fly using\\na deep neural network model trained using examples of spam and ham;\\nthis makes it an online, model-based, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and\\ntype of supervision they get during training. There are four major\\ncategories: supervised learning, unsupervised learning, semisupervised\\nlearning, and Reinforcement Learning.\\nSupervised learning\\nIn supervised learning, the training set you feed to the algorithm includes\\nthe desired solutions, called labels (Figure 1-5).\\nFigure 1-5. A labeled training set for spam classification (an example of supervised learning)\\nA typical supervised learning task is classification. The spam filter is a\\ngood example of this: it is trained with many example emails along with\\ntheir class (spam or ham), and it must learn how to classify new emails.\\nAnother typical task is to predict a target numeric value, such as the price\\nof a car, given a set of features (mileage, age, brand, etc.) called\\npredictors. This sort of task is called regression (Figure 1-6).  To train the\\nsystem, you need to give it many examples of cars, including both their\\npredictors and their labels (i.e., their prices).\\nNOTE\\nIn Machine Learning an attribute is a data type (e.g., “mileage”), while a feature has\\nseveral meanings, depending on the context, but generally means an attribute plus its\\nvalue (e.g., “mileage = 15,000”). Many people use the words attribute and feature\\ninterchangeably.\\nNote that some regression algorithms can be used for classification as\\nwell, and vice versa. For example, Logistic Regression is commonly used\\nfor classification, as it can output a value that corresponds to the\\nprobability of belonging to a given class (e.g., 20% chance of being spam).\\nFigure 1-6. A regression problem: predict a value, given an input feature (there are usually\\nmultiple input features, and sometimes multiple output values)\\nHere are some of the most important supervised learning algorithms\\n(covered in this book):\\nk-Nearest Neighbors\\n1\\nLinear Regression\\nLogistic Regression\\nSupport Vector Machines (SVMs)\\nDecision Trees and Random Forests\\nNeural networks\\nUnsupervised learning\\nIn unsupervised learning, as you might guess, the training data is\\nunlabeled (Figure 1-7). The system tries to learn without a teacher.\\nFigure 1-7. An unlabeled training set for unsupervised learning\\nHere are some of the most important unsupervised learning algorithms\\n(most of these are covered in Chapters 8 and 9):\\nClustering\\nK-Means\\nDBSCAN\\nHierarchical Cluster Analysis (HCA)\\nAnomaly detection and novelty detection\\n2\\nOne-class SVM\\nIsolation Forest\\nVisualization and dimensionality reduction\\nPrincipal Component Analysis (PCA)\\nKernel PCA\\nLocally Linear Embedding (LLE)\\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\\nAssociation rule learning\\nApriori\\nEclat\\nFor example, say you have a lot of data about your blog’s visitors. You\\nmay want to run a clustering algorithm to try to detect groups of similar\\nvisitors (Figure 1-8). At no point do you tell the algorithm which group a\\nvisitor belongs to: it finds those connections without your help. For\\nexample, it might notice that 40% of your visitors are males who love\\ncomic books and generally read your blog in the evening, while 20% are\\nyoung sci-fi lovers who visit during the weekends. If you use a\\nhierarchical clustering algorithm, it may also subdivide each group into\\nsmaller groups. This may help you target your posts for each group.\\nFigure 1-8. Clustering\\nVisualization algorithms are also good examples of unsupervised learning\\nalgorithms: you feed them a lot of complex and unlabeled data, and they\\noutput a 2D or 3D representation of your data that can easily be plotted\\n(Figure 1-9). These algorithms try to preserve as much structure as they\\ncan (e.g., trying to keep separate clusters in the input space from\\noverlapping in the visualization) so that you can understand how the data\\nis organized and perhaps identify unsuspected patterns.\\nFigure 1-9. Example of a t-SNE visualization highlighting semantic clusters\\nA related task is dimensionality reduction, in which the goal is to simplify\\nthe data without losing too much information. One way to do this is to\\nmerge several correlated features into one. For example, a car’s mileage\\nmay be strongly correlated with its age, so the dimensionality reduction\\nalgorithm will merge them into one feature that represents the car’s wear\\nand tear. This is called feature extraction.\\nTIP\\nIt is often a good idea to try to reduce the dimension of your training data using a\\ndimensionality reduction algorithm before you feed it to another Machine Learning\\nalgorithm (such as a supervised learning algorithm). It will run much faster, the data\\nwill take up less disk and memory space, and in some cases it may also perform\\nbetter.\\nYet another important unsupervised task is anomaly detection—for\\nexample, detecting unusual credit card transactions to prevent fraud,\\n3\\ncatching manufacturing defects, or automatically removing outliers from a\\ndataset before feeding it to another learning algorithm. The system is\\nshown mostly normal instances during training, so it learns to recognize\\nthem; then, when it sees a new instance, it can tell whether it looks like a\\nnormal one or whether it is likely an anomaly (see Figure 1-10). A very\\nsimilar task is novelty detection: it aims to detect new instances that look\\ndifferent from all instances in the training set. This requires having a very\\n“clean” training set, devoid of any instance that you would like the\\nalgorithm to detect. For example, if you have thousands of pictures of\\ndogs, and 1% of these pictures represent Chihuahuas, then a novelty\\ndetection algorithm should not treat new pictures of Chihuahuas as\\nnovelties. On the other hand, anomaly detection algorithms may consider\\nthese dogs as so rare and so different from other dogs that they would\\nlikely classify them as anomalies (no offense to Chihuahuas).\\nFigure 1-10. Anomaly detection\\nFinally, another common unsupervised task is association rule learning, in\\nwhich the goal is to dig into large amounts of data and discover interesting\\nrelations between attributes. For example, suppose you own a\\nsupermarket. Running an association rule on your sales logs may reveal\\nthat people who purchase barbecue sauce and potato chips also tend to buy\\nsteak. Thus, you may want to place these items close to one another.\\nSemisupervised learning\\nSince labeling data is usually time-consuming and costly, you will often\\nhave plenty of unlabeled instances, and few labeled instances. Some\\nalgorithms can deal with data that’s partially labeled. This is called\\nsemisupervised learning (Figure 1-11).\\nFigure 1-11. Semisupervised learning with two classes (triangles and squares): the unlabeled\\nexamples (circles) help classify a new instance (the cross) into the triangle class rather than the\\nsquare class, even though it is closer to the labeled squares\\nSome photo-hosting services, such as Google Photos, are good examples\\nof this. Once you upload all your family photos to the service, it\\nautomatically recognizes that the same person A shows up in photos 1, 5,\\nand 11, while another person B shows up in photos 2, 5, and 7. This is the\\nunsupervised part of the algorithm (clustering). Now all the system needs\\nis for you to tell it who these people are. Just add one label per person  and\\nit is able to name everyone in every photo, which is useful for searching\\nphotos.\\nMost semisupervised learning algorithms are combinations of\\nunsupervised and supervised algorithms. For example, deep belief\\nnetworks (DBNs) are based on unsupervised components called restricted\\nBoltzmann machines (RBMs) stacked on top of one another. RBMs are\\n4\\ntrained sequentially in an unsupervised manner, and then the whole system\\nis fine-tuned using supervised learning techniques.\\nReinforcement Learning\\nReinforcement Learning isag a very different beast. The learning system,\\ncalled an agent in this context, can observe the environment, select and\\nperform actions, and get rewards in return (or penalties in the form of\\nnegative rewards, as shown in Figure 1-12). It must then learn by itself\\nwhat is the best strategy, called a policy, to get the most reward over time.\\nA policy defines what action the agent should choose when it is in a given\\nsituation.\\nFigure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms\\nto learn how to walk. DeepMind’s AlphaGo program is also a good\\nexample of Reinforcement Learning: it made the headlines in May 2017\\nwhen it beat the world champion Ke Jie at the game of Go. It learned its\\nwinning policy by analyzing millions of games, and then playing many\\ngames against itself. Note that learning was turned off during the games\\nagainst the champion; AlphaGo was just applying the policy it had\\nlearned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or\\nnot the system can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning, the system is incapable of learning incrementally: it\\nmust be trained using all the available data. This will generally take a lot\\nof time and computing resources, so it is typically done offline. First the\\nsystem is trained, and then it is launched into production and runs without\\nlearning anymore; it just applies what it has learned. This is called offline\\nlearning.\\nIf you want a batch learning system to know about new data (such as a new\\ntype of spam), you need to train a new version of the system from scratch\\non the full dataset (not just the new data, but also the old data), then stop\\nthe old system and replace it with the new one.\\nFortunately, the whole process of training, evaluating, and launching a\\nMachine Learning system can be automated fairly easily (as shown in\\nFigure 1-3), so even a batch learning system can adapt to change. Simply\\nupdate the data and train a new version of the system from scratch as often\\nas needed.\\nThis solution is simple and often works fine, but training using the full set\\nof data can take many hours, so you would typically train a new system\\nonly every 24 hours or even just weekly. If your system needs to adapt to\\nrapidly changing data (e.g., to predict stock prices), then you need a more\\nreactive solution.\\nAlso, training on the full set of data requires a lot of computing resources\\n(CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have\\na lot of data and you automate your system to train from scratch every day,\\nit will end up costing you a lot of money. If the amount of data is huge, it\\nmay even be impossible to use a batch learning algorithm.\\nFinally, if your system needs to be able to learn autonomously and it has\\nlimited resources (e.g., a smartphone application or a rover on Mars), then\\ncarrying around large amounts of training data and taking up a lot of\\nresources to train for hours every day is a showstopper.\\nFortunately, a better option in all these cases is to use algorithms that are\\ncapable of learning incrementally.\\nOnline learning\\nIn online learning, you train the system incrementally by feeding it data\\ninstances sequentially, either individually or in small groups called mini-\\nbatches. Each learning step is fast and cheap, so the system can learn\\nabout new data on the fly, as it arrives (see Figure 1-13).\\nFigure 1-13. In online learning, a model is trained and launched into production, and then it\\nkeeps learning as new data comes in\\nOnline learning is great for systems that receive data as a continuous flow\\n(e.g., stock prices) and need to adapt to change rapidly or autonomously. It\\nis also a good option if you have limited computing resources: once an\\nonline learning system has learned about new data instances, it does not\\nneed them anymore, so you can discard them (unless you want to be able\\nto roll back to a previous state and “replay” the data). This can save a huge\\namount of space.\\nOnline learning algorithms can also be used to train systems on huge\\ndatasets that cannot fit in one machine’s main memory (this is called out-\\nof-core learning). The algorithm loads part of the data, runs a training step\\non that data, and repeats the process until it has run on all of the data (see\\nFigure 1-14).\\nWARNING\\nOut-of-core learning is usually done offline (i.e., not on the live system), so online\\nlearning can be a confusing name. Think of it as incremental learning.\\nOne important parameter of online learning systems is how fast they\\nshould adapt to changing data: this is called the learning rate. If you set a\\nhigh learning rate, then your system will rapidly adapt to new data, but it\\nwill also tend to quickly forget the old data (you don’t want a spam filter\\nto flag only the latest kinds of spam it was shown). Conversely, if you set a\\nlow learning rate, the system will have more inertia; that is, it will learn\\nmore slowly, but it will also be less sensitive to noise in the new data or to\\nsequences of nonrepresentative data points (outliers).\\nFigure 1-14. Using online learning to handle huge datasets\\nA big challenge with online learning is that if bad data is fed to the\\nsystem, the system’s performance will gradually decline. If it’s a live\\nsystem, your clients will notice. For example, bad data could come from a\\nmalfunctioning sensor on a robot, or from someone spamming a search\\nengine to try to rank high in search results. To reduce this risk, you need to\\nmonitor your system closely and promptly switch learning off (and\\npossibly revert to a previously working state) if you detect a drop in\\nperformance. You may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they\\ngeneralize. Most Machine Learning tasks are about making predictions.\\nThis means that given a number of training examples, the system needs to\\nbe able to make good predictions for (generalize to) examples it has never\\nseen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning\\nand model-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If\\nyou were to create a spam filter this way, it would just flag all emails that\\nare identical to emails that have already been flagged by users—not the\\nworst solution, but certainly not the best.\\nInstead of just flagging emails that are identical to known spam emails,\\nyour spam filter could be programmed to also flag emails that are very\\nsimilar to known spam emails. This requires a measure of similarity\\nbetween two emails. A (very basic) similarity measure between two\\nemails could be to count the number of words they have in common. The\\nsystem would flag an email as spam if it has many words in common with\\na known spam email.\\nThis is called instance-based learning: the system learns the examples by\\nheart, then generalizes to new cases by using a similarity measure to\\ncompare them to the learned examples (or a subset of them). For example,\\nin Figure 1-15 the new instance would be classified as a triangle because\\nthe majority of the most similar instances belong to that class.\\nFigure 1-15. Instance-based learning\\nModel-based learning\\nAnother way to generalize from a set of examples is to build a model of\\nthese examples and then use that model to make predictions. This is called\\nmodel-based learning (Figure 1-16).\\nFigure 1-16. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so\\nyou download the Better Life Index data from the OECD’s website and\\nstats about gross domestic product (GDP) per capita from the IMF’s\\nwebsite. Then you join the tables and sort by GDP per capita. Table 1-1\\nshows an excerpt of what you get.\\nTable 1-1. Does money make people happier?\\nCountry\\nGDP per capita (USD) Life satisfaction\\nHungary\\n12,240\\n4.9\\nKorea\\n27,195\\n5.8\\nFrance\\n37,675\\n6.5\\nAustralia\\n50,962\\n7.3\\nUnited States 55,805\\n7.2\\nLet’s plot the data for these countries (Figure 1-17).\\nFigure 1-17. Do you see a trend here?\\nThere does seem to be a trend here! Although the data is noisy (i.e., partly\\nrandom), it looks like life satisfaction goes up more or less linearly as the\\ncountry’s GDP per capita increases. So you decide to model life\\nsatisfaction as a linear function of GDP per capita. This step is called\\nmodel selection: you selected a linear model of life satisfaction with just\\none attribute, GDP per capita (Equation 1-1).\\nEquation 1-1. A simple linear model\\nlife_satisfaction = θ0 + θ1 × GDP_per_capita\\nThis model has two model parameters, θ  and θ .  By tweaking these\\nparameters, you can make your model represent any linear function, as\\nshown in Figure 1-18.\\n0\\n1 5\\nFigure 1-18. A few possible linear models\\nBefore you can use your model, you need to define the parameter values θ\\nand θ . How can you know which values will make your model perform\\nbest? To answer this question, you need to specify a performance measure.\\nYou can either define a utility function (or fitness function) that measures\\nhow good your model is, or you can define a cost function that measures\\nhow bad it is. For Linear Regression problems, people typically use a cost\\nfunction that measures the distance between the linear model’s predictions\\nand the training examples; the objective is to minimize this distance.\\nThis is where the Linear Regression algorithm comes in: you feed it your\\ntraining examples, and it finds the parameters that make the linear model\\nfit best to your data. This is called training the model. In our case, the\\nalgorithm finds that the optimal parameter values are θ  = 4.85 and θ  =\\n4.91 × 10\\n.\\n0\\n1\\n0\\n1\\n–5\\nWARNING\\nConfusingly, the same word “model” can refer to a type of model (e.g., Linear\\nRegression), to a fully specified model architecture (e.g., Linear Regression with one\\ninput and one output), or to the final trained model ready to be used for predictions\\n(e.g., Linear Regression with one input and one output, using θ  = 4.85 and θ  = 4.91\\n× 10\\n). Model selection consists in choosing the type of model and fully specifying\\nits architecture. Training a model means running an algorithm to find the model\\nparameters that will make it best fit the training data (and hopefully make good\\npredictions on new data).\\nNow the model fits the training data as closely as possible (for a linear\\nmodel), as you can see in Figure 1-19.\\nFigure 1-19. The linear model that fits the training data best\\nYou are finally ready to run the model to make predictions. For example,\\nsay you want to know how happy Cypriots are, and the OECD data does\\nnot have the answer. Fortunately, you can use your model to make a good\\nprediction: you look up Cyprus’s GDP per capita, find $22,587, and then\\napply your model and find that life satisfaction is likely to be somewhere\\naround 4.85 + 22,587 × 4.91 × 10  = 5.96.\\n0\\n1\\n–5\\n-5\\nTo whet your appetite, Example 1-1 shows the Python code that loads the\\ndata, prepares it,  creates a scatterplot for visualization, and then trains a\\nlinear model and makes a prediction.\\nExample 1-1. Training and running a linear model using Scikit-Learn\\nimport matplotlib.pyplot as plt \\nimport numpy as np \\nimport pandas as pd \\nimport sklearn.linear_model \\n \\n# Load the data \\noecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=\\',\\') \\ngdp_per_capita = \\npd.read_csv(\"gdp_per_capita.csv\",thousands=\\',\\',delimiter=\\'\\\\t\\', \\n                             encoding=\\'latin1\\', na_values=\"n/a\") \\n \\n \\n# Prepare the data \\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita) \\nX = np.c_[country_stats[\"GDP per capita\"]] \\ny = np.c_[country_stats[\"Life satisfaction\"]] \\n \\n# Visualize the data \\ncountry_stats.plot(kind=\\'scatter\\', x=\"GDP per capita\", y=\\'Life \\nsatisfaction\\') \\nplt.show() \\n \\n# Select a linear model \\nmodel = sklearn.linear_model.LinearRegression() \\n \\n# Train the model \\nmodel.fit(X, y) \\n \\n# Make a prediction for Cyprus \\nX_new = [[22587]]  # Cyprus\\'s GDP per capita \\nprint(model.predict(X_new)) # outputs [[ 5.96242338]]\\n6\\n7\\nNOTE\\nIf you had used an instance-based learning algorithm instead, you would have found\\nthat Slovenia has the closest GDP per capita to that of Cyprus ($20,732), and since\\nthe OECD data tells us that Slovenians’ life satisfaction is 5.7, you would have\\npredicted a life satisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\\ntwo next-closest countries, you will find Portugal and Spain with life satisfactions of\\n5.1 and 6.5, respectively. Averaging these three values, you get 5.77, which is pretty\\nclose to your model-based prediction. This simple algorithm is called k-Nearest\\nNeighbors regression (in this example, k = 3).\\nReplacing the Linear Regression model with k-Nearest Neighbors regression in the\\nprevious code is as simple as replacing these two lines:\\nimport sklearn.linear_model \\nmodel = sklearn.linear_model.LinearRegression()\\nwith these two:\\nimport sklearn.neighbors \\nmodel = sklearn.neighbors.KNeighborsRegressor( \\n    n_neighbors=3)\\nIf all went well, your model will make good predictions. If not, you may\\nneed to use more attributes (employment rate, health, air pollution, etc.),\\nget more or better-quality training data, or perhaps select a more powerful\\nmodel (e.g., a Polynomial Regression model).\\nIn summary:\\nYou studied the data.\\nYou selected a model.\\nYou trained it on the training data (i.e., the learning algorithm\\nsearched for the model parameter values that minimize a cost\\nfunction).\\nFinally, you applied the model to make predictions on new cases\\n(this is called inference), hoping that this model will generalize\\nwell.\\nThis is what a typical Machine Learning project looks like. In Chapter 2\\nyou will experience this firsthand by going through a project end to end.\\nWe have covered a lot of ground so far: you now know what Machine\\nLearning is really about, why it is useful, what some of the most common\\ncategories of ML systems are, and what a typical project workflow looks\\nlike. Now let’s look at what can go wrong in learning and prevent you from\\nmaking accurate predictions.\\nMain Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it\\non some data, the two things that can go wrong are “bad algorithm” and\\n“bad data.” Let’s start with examples of bad data.\\nInsufficient Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an\\napple and say “apple” (possibly repeating this procedure a few times).\\nNow the child is able to recognize apples in all sorts of colors and shapes.\\nGenius.\\nMachine Learning is not quite there yet; it takes a lot of data for most\\nMachine Learning algorithms to work properly. Even for very simple\\nproblems you typically need thousands of examples, and for complex\\nproblems such as image or speech recognition you may need millions of\\nexamples (unless you can reuse parts of an existing model).\\nTHE UNREASONABLE EFFECTIVENESS OF DATA\\nIn a famous paper published in 2001, Microsoft researchers Michele\\nBanko and Eric Brill showed that very different Machine Learning\\nalgorithms, including fairly simple ones, performed almost identically\\nwell on a complex problem of natural language disambiguation  once\\nthey were given enough data (as you can see in Figure 1-20).\\nFigure 1-20. The importance of data versus algorithms\\nAs the authors put it, “these results suggest that we may want to\\nreconsider the trade-off between spending time and money on\\nalgorithm development versus spending it on corpus development.”\\n8\\n9\\nThe idea that data matters more than algorithms for complex problems\\nwas further popularized by Peter Norvig et al. in a paper titled “The\\nUnreasonable Effectiveness of Data”, published in 2009.\\n It should be\\nnoted, however, that small- and medium-sized datasets are still very\\ncommon, and it is not always easy or cheap to get extra training data —\\nso don’t abandon algorithms just yet.\\nNonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be\\nrepresentative of the new cases you want to generalize to. This is true\\nwhether you use instance-based learning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear\\nmodel was not perfectly representative; a few countries were missing.\\nFigure 1-21 shows what the data looks like when you add the missing\\ncountries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old\\nmodel is represented by the dotted line. As you can see, not only does\\nadding a few missing countries significantly alter the model, but it makes\\nit clear that such a simple linear model is probably never going to work\\nwell. It seems that very rich countries are not happier than moderately rich\\ncountries (in fact, they seem unhappier), and conversely some poor\\ncountries seem happier than many rich countries.\\n10\\nBy using a nonrepresentative training set, we trained a model that is\\nunlikely to make accurate predictions, especially for very poor and very\\nrich countries.\\nIt is crucial to use a training set that is representative of the cases you\\nwant to generalize to. This is often harder than it sounds: if the sample is\\ntoo small, you will have sampling noise (i.e., nonrepresentative data as a\\nresult of chance), but even very large samples can be nonrepresentative if\\nthe sampling method is flawed. This is called sampling bias.\\nEXAMPLES OF SAMPLING BIAS\\nPerhaps the most famous example of sampling bias happened during\\nthe US presidential election in 1936, which pitted Landon against\\nRoosevelt: the Literary Digest conducted a very large poll, sending\\nmail to about 10 million people. It got 2.4 million answers, and\\npredicted with high confidence that Landon would get 57% of the\\nvotes. Instead, Roosevelt won with 62% of the votes. The flaw was in\\nthe Literary Digest’s sampling method:\\nFirst, to obtain the addresses to send the polls to, the Literary\\nDigest used telephone directories, lists of magazine\\nsubscribers, club membership lists, and the like. All of these\\nlists tended to favor wealthier people, who were more likely\\nto vote Republican (hence Landon).\\nSecond, less than 25% of the people who were polled\\nanswered. Again this introduced a sampling bias, by\\npotentially ruling out people who didn’t care much about\\npolitics, people who didn’t like the Literary Digest, and other\\nkey groups. This is a special type of sampling bias called\\nnonresponse bias.\\nHere is another example: say you want to build a system to recognize\\nfunk music videos. One way to build your training set is to search for\\n“funk music” on YouTube and use the resulting videos. But this\\nassumes that YouTube’s search engine returns a set of videos that are\\nrepresentative of all the funk music videos on YouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you\\nlive in Brazil you will get a lot of “funk carioca” videos, which sound\\nnothing like James Brown). On the other hand, how else can you get a\\nlarge training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g.,\\ndue to poor-quality measurements), it will make it harder for the system to\\ndetect the underlying patterns, so your system is less likely to perform\\nwell. It is often well worth the effort to spend time cleaning up your\\ntraining data. The truth is, most data scientists spend a significant part of\\ntheir time doing just that. The following are a couple of examples of when\\nyou’d want to clean up training data:\\nIf some instances are clearly outliers, it may help to simply\\ndiscard them or try to fix the errors manually.\\nIf some instances are missing a few features (e.g., 5% of your\\ncustomers did not specify their age), you must decide whether you\\nwant to ignore this attribute altogether, ignore these instances, fill\\nin the missing values (e.g., with the median age), or train one\\nmodel with the feature and one model without it.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Your system will only be\\ncapable of learning if the training data contains enough relevant features\\nand not too many irrelevant ones. A critical part of the success of a\\nMachine Learning project is coming up with a good set of features to train\\non. This process, called feature engineering, involves the following steps:\\nFeature selection (selecting the most useful features to train on\\namong existing features)\\nFeature extraction (combining existing features to produce a\\nmore useful one —as we saw earlier, dimensionality reduction\\nalgorithms can help)\\nCreating new features by gathering new data\\nNow that we have looked at many examples of bad data, let’s look at a\\ncouple of examples of bad algorithms.\\nOverfitting the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. You\\nmight be tempted to say that all taxi drivers in that country are thieves.\\nOvergeneralizing is something that we humans do all too often, and\\nunfortunately machines can fall into the same trap if we are not careful. In\\nMachine Learning this is called overfitting: it means that the model\\nperforms well on the training data, but it does not generalize well.\\nFigure 1-22 shows an example of a high-degree polynomial life\\nsatisfaction model that strongly overfits the training data. Even though it\\nperforms much better on the training data than the simple linear model,\\nwould you really trust its predictions?\\nFigure 1-22. Overfitting the training data\\nComplex models such as deep neural networks can detect subtle patterns\\nin the data, but if the training set is noisy, or if it is too small (which\\nintroduces sampling noise), then the model is likely to detect patterns in\\nthe noise itself. Obviously these patterns will not generalize to new\\ninstances. For example, say you feed your life satisfaction model many\\nmore attributes, including uninformative ones such as the country’s name.\\nIn that case, a complex model may detect patterns like the fact that all\\ncountries in the training data with a w in their name have a life satisfaction\\ngreater than 7: New Zealand (7.3), Norway (7.4), Sweden (7.2), and\\nSwitzerland (7.5). How confident are you that the w-satisfaction rule\\ngeneralizes to Rwanda or Zimbabwe? Obviously this pattern occurred in\\nthe training data by pure chance, but the model has no way to tell whether\\na pattern is real or simply the result of noise in the data.\\nWARNING\\nOverfitting happens when the model is too complex relative to the amount and\\nnoisiness of the training data. Here are possible solutions:\\nSimplify the model by selecting one with fewer parameters (e.g., a linear\\nmodel rather than a high-degree polynomial model), by reducing the\\nnumber of attributes in the training data, or by constraining the model.\\nGather more training data.\\nReduce the noise in the training data (e.g., fix data errors and remove\\noutliers).\\nConstraining a model to make it simpler and reduce the risk of overfitting\\nis called regularization. For example, the linear model we defined earlier\\nhas two parameters, θ  and θ . This gives the learning algorithm two\\ndegrees of freedom to adapt the model to the training data: it can tweak\\nboth the height (θ ) and the slope (θ ) of the line. If we forced θ  = 0, the\\nalgorithm would have only one degree of freedom and would have a much\\nharder time fitting the data properly: all it could do is move the line up or\\ndown to get as close as possible to the training instances, so it would end\\nup around the mean. A very simple model indeed! If we allow the\\nalgorithm to modify θ  but we force it to keep it small, then the learning\\nalgorithm will effectively have somewhere in between one and two\\ndegrees of freedom. It will produce a model that’s simpler than one with\\ntwo degrees of freedom, but more complex than one with just one. You\\nwant to find the right balance between fitting the training data perfectly\\nand keeping the model simple enough to ensure that it will generalize\\nwell.\\nFigure 1-23 shows three models. The dotted line represents the original\\nmodel that was trained on the countries represented as circles (without the\\ncountries represented as squares), the dashed line is our second model\\n0\\n1\\n0\\n1\\n1\\n1\\ntrained with all countries (circles and squares), and the solid line is a\\nmodel trained with the same data as the first model but with a\\nregularization constraint. You can see that regularization forced the model\\nto have a smaller slope: this model does not fit the training data (circles)\\nas well as the first model, but it actually generalizes better to new\\nexamples that it did not see during training (squares).\\nFigure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by\\na hyperparameter. A hyperparameter is a parameter of a learning\\nalgorithm (not of the model). As such, it is not affected by the learning\\nalgorithm itself; it must be set prior to training and remains constant\\nduring training. If you set the regularization hyperparameter to a very\\nlarge value, you will get an almost flat model (a slope close to zero); the\\nlearning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an\\nimportant part of building a Machine Learning system (you will see a\\ndetailed example in the next chapter).\\nUnderfitting the Training Data\\nAs you might guess, underfitting is the opposite of overfitting: it occurs\\nwhen your model is too simple to learn the underlying structure of the\\ndata. For example, a linear model of life satisfaction is prone to underfit;\\nreality is just more complex than the model, so its predictions are bound to\\nbe inaccurate, even on the training examples.\\nHere are the main options for fixing this problem:\\nSelect a more powerful model, with more parameters.\\nFeed better features to the learning algorithm (feature\\nengineering).\\nReduce the constraints on the model (e.g., reduce the\\nregularization hyperparameter).\\nStepping Back\\nBy now you know a lot about Machine Learning. However, we went\\nthrough so many concepts that you may be feeling a little lost, so let’s step\\nback and look at the big picture:\\nMachine Learning is about making machines get better at some\\ntask by learning from data, instead of having to explicitly code\\nrules.\\nThere are many different types of ML systems: supervised or not,\\nbatch or online, instance-based or model-based.\\nIn an ML project you gather data in a training set, and you feed\\nthe training set to a learning algorithm. If the algorithm is model-\\nbased, it tunes some parameters to fit the model to the training set\\n(i.e., to make good predictions on the training set itself), and then\\nhopefully it will be able to make good predictions on new cases as\\nwell. If the algorithm is instance-based, it just learns the\\nexamples by heart and generalizes to new instances by using a\\nsimilarity measure to compare them to the learned instances.\\nThe system will not perform well if your training set is too small,\\nor if the data is not representative, is noisy, or is polluted with\\nirrelevant features (garbage in, garbage out). Lastly, your model\\nneeds to be neither too simple (in which case it will underfit) nor\\ntoo complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a\\nmodel, you don’t want to just “hope” it generalizes to new cases. You want\\nto evaluate it and fine-tune it if necessary. Let’s see how to do that.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to\\nactually try it out on new cases. One way to do that is to put your model in\\nproduction and monitor how well it performs. This works well, but if your\\nmodel is horribly bad, your users will complain—not the best idea.\\nA better option is to split your data into two sets: the training set and the\\ntest set. As these names imply, you train your model using the training set,\\nand you test it using the test set. The error rate on new cases is called the\\ngeneralization error (or out-of-sample error), and by evaluating your\\nmodel on the test set, you get an estimate of this error. This value tells you\\nhow well your model will perform on instances it has never seen before.\\nIf the training error is low (i.e., your model makes few mistakes on the\\ntraining set) but the generalization error is high, it means that your model\\nis overfitting the training data.\\nTIP\\nIt is common to use 80% of the data for training and hold out 20% for testing.\\nHowever, this depends on the size of the dataset: if it contains 10 million instances,\\nthen holding out 1% means your test set will contain 100,000 instances, probably\\nmore than enough to get a good estimate of the generalization error.\\nHyperparameter Tuning and Model Selection\\nEvaluating a model is simple enough: just use a test set. But suppose you\\nare hesitating between two types of models (say, a linear model and a\\npolynomial model): how can you decide between them? One option is to\\ntrain both and compare how well they generalize using the test set.\\nNow suppose that the linear model generalizes better, but you want to\\napply some regularization to avoid overfitting. The question is, how do\\nyou choose the value of the regularization hyperparameter? One option is\\nto train 100 different models using 100 different values for this\\nhyperparameter. Suppose you find the best hyperparameter value that\\nproduces a model with the lowest generalization error —say, just 5% error.\\nYou launch this model into production, but unfortunately it does not\\nperform as well as expected and produces 15% errors. What just\\nhappened?\\nThe problem is that you measured the generalization error multiple times\\non the test set, and you adapted the model and hyperparameters to produce\\nthe best model for that particular set. This means that the model is\\nunlikely to perform as well on new data.\\nA common solution to this problem is called holdout validation: you\\nsimply hold out part of the training set to evaluate several candidate\\nmodels and select the best one. The new held-out set is called the\\nvalidation set (or sometimes the development set, or dev set). More\\nspecifically, you train multiple models with various hyperparameters on\\nthe reduced training set (i.e., the full training set minus the validation set),\\nand you select the model that performs best on the validation set. After\\nthis holdout validation process, you train the best model on the full\\ntraining set (including the validation set), and this gives you the final\\nmodel. Lastly, you evaluate this final model on the test set to get an\\nestimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is\\ntoo small, then model evaluations will be imprecise: you may end up\\nselecting a suboptimal model by mistake. Conversely, if the validation set\\nis too large, then the remaining training set will be much smaller than the\\nfull training set. Why is this bad? Well, since the final model will be\\ntrained on the full training set, it is not ideal to compare candidate models\\ntrained on a much smaller training set. It would be like selecting the\\nfastest sprinter to participate in a marathon. One way to solve this problem\\nis to perform repeated cross-validation, using many small validation sets.\\nEach model is evaluated once per validation set after it is trained on the\\nrest of the data. By averaging out all the evaluations of a model, you get a\\nmuch more accurate measure of its performance. There is a drawback,\\nhowever: the training time is multiplied by the number of validation sets.\\nData Mismatch\\nIn some cases, it’s easy to get a large amount of data for training, but this\\ndata probably won’t be perfectly representative of the data that will be\\nused in production. For example, suppose you want to create a mobile app\\nto take pictures of flowers and automatically determine their species. You\\ncan easily download millions of pictures of flowers on the web, but they\\nwon’t be perfectly representative of the pictures that will actually be taken\\nusing the app on a mobile device. Perhaps you only have 10,000\\nrepresentative pictures (i.e., actually taken with the app). In this case, the\\nmost important rule to remember is that the validation set and the test set\\nmust be as representative as possible of the data you expect to use in\\nproduction, so they should be composed exclusively of representative\\npictures: you can shuffle them and put half in the validation set and half in\\nthe test set (making sure that no duplicates or near-duplicates end up in\\nboth sets). But after training your model on the web pictures, if you\\nobserve that the performance of the model on the validation set is\\ndisappointing, you will not know whether this is because your model has\\noverfit the training set, or whether this is just due to the mismatch between\\nthe web pictures and the mobile app pictures. One solution is to hold out\\nsome of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set. After the model is trained (on the\\ntraining set, not on the train-dev set), you can evaluate it on the train-dev\\nset. If it performs well, then the model is not overfitting the training set. If\\nit performs poorly on the validation set, the problem must be coming from\\nthe data mismatch. You can try to tackle this problem by preprocessing the\\nweb images to make them look more like the pictures that will be taken by\\nthe mobile app, and then retraining the model. Conversely, if the model\\nperforms poorly on the train-dev set, then it must have overfit the training\\nset, so you should try to simplify or regularize the model, get more\\ntraining data, and clean up the training data.\\nNO FREE LUNCH THEOREM\\nA model is a simplified version of the observations. The\\nsimplifications are meant to discard the superfluous details that are\\nunlikely to generalize to new instances. To decide what data to discard\\nand what data to keep, you must make assumptions. For example, a\\nlinear model makes the assumption that the data is fundamentally\\nlinear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper,  David Wolpert demonstrated that if you\\nmake absolutely no assumption about the data, then there is no reason\\nto prefer one model over any other. This is called the No Free Lunch\\n(NFL) theorem. For some datasets the best model is a linear model,\\nwhile for other datasets it is a neural network. There is no model that\\nis a priori guaranteed to work better (hence the name of the theorem).\\nThe only way to know for sure which model is best is to evaluate them\\nall. Since this is not possible, in practice you make some reasonable\\nassumptions about the data and evaluate only a few reasonable models.\\nFor example, for simple tasks you may evaluate linear models with\\nvarious levels of regularization, and for a complex problem you may\\nevaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in\\nMachine Learning. In the next chapters we will dive deeper and write\\nmore code, but before we do, make sure you know how to answer the\\nfollowing questions:\\n1. How would you define Machine Learning?\\n11\\n2. Can you name four types of problems where it shines?\\n3. What is a labeled training set?\\n4. What are the two most common supervised tasks?\\n5. Can you name four common unsupervised tasks?\\n6. What type of Machine Learning algorithm would you use to allow\\na robot to walk in various unknown terrains?\\n7. What type of algorithm would you use to segment your customers\\ninto multiple groups?\\n8. Would you frame the problem of spam detection as a supervised\\nlearning problem or an unsupervised learning problem?\\n9. What is an online learning system?\\n10. What is out-of-core learning?\\n11. What type of learning algorithm relies on a similarity measure to\\nmake predictions?\\n12. What is the difference between a model parameter and a learning\\nalgorithm’s hyperparameter?\\n13. What do model-based learning algorithms search for? What is the\\nmost common strategy they use to succeed? How do they make\\npredictions?\\n14. Can you name four of the main challenges in Machine Learning?\\n15. If your model performs great on the training data but generalizes\\npoorly to new instances, what is happening? Can you name three\\npossible solutions?\\n16. What is a test set, and why would you want to use it?\\n17. What is the purpose of a validation set?\\n18. What is the train-dev set, when do you need it, and how do you\\nuse it?\\n19. What can go wrong if you tune hyperparameters using the test\\nset?\\nSolutions to these exercises are available in Appendix A.\\n1  Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he\\nwas studying the fact that the children of tall people tend to be shorter than their parents.\\nSince the children were shorter, he called this regression to the mean. This name was then\\napplied to the methods he used to analyze correlations between variables.\\n2  Some neural network architectures can be unsupervised, such as autoencoders and\\nrestricted Boltzmann machines. They can also be semisupervised, such as in deep belief\\nnetworks and unsupervised pretraining.\\n3  Notice how animals are rather well separated from vehicles and how horses are close to\\ndeer but far from birds. Figure reproduced with permission from Richard Socher et al.,\\n“Zero-Shot Learning Through Cross-Modal Transfer,” Proceedings of the 26th\\nInternational Conference on Neural Information Processing Systems 1 (2013): 935–943.\\n4  That’s when the system works perfectly. In practice it often creates a few clusters per\\nperson, and sometimes mixes up two people who look alike, so you may need to provide a\\nfew labels per person and manually clean up some clusters.\\n5  By convention, the Greek letter θ (theta) is frequently used to represent model parameters.\\n6  The prepare_country_stats() function’s definition is not shown here (see this\\nchapter’s Jupyter notebook if you want all the gory details). It’s just boring pandas code\\nthat joins the life satisfaction data from the OECD with the GDP per capita data from the\\nIMF.\\n7  It’s OK if you don’t understand all the code yet; we will present Scikit-Learn in the\\nfollowing chapters.\\n8  For example, knowing whether to write “to,” “two,” or “too,” depending on the context.\\n9  Figure reproduced with permission from Michele Banko and Eric Brill, “Scaling to Very\\nVery Large Corpora for Natural Language Disambiguation,” Proceedings of the 39th\\nAnnual Meeting of the Association for Computational Linguistics (2001): 26–33.\\n10  Peter Norvig et al., “The Unreasonable Effectiveness of Data,” IEEE Intelligent Systems\\n24, no. 2 (2009): 8–12.\\n11  David Wolpert, “The Lack of A Priori Distinctions Between Learning Algorithms,” Neural\\nComputation 8, no. 7 (1996): 1341–1390.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["sentences=texts1+texts2"],"metadata":{"id":"tz2fwsjrtn-N","executionInfo":{"status":"ok","timestamp":1726605285752,"user_tz":-180,"elapsed":185,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["## Text Pre-processing"],"metadata":{"id":"vJsPGKmJAQY9"}},{"cell_type":"code","source":["nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"wNGG-S98AKvL","executionInfo":{"status":"ok","timestamp":1726605286868,"user_tz":-180,"elapsed":1300,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# define the preprocess function\n","def preprocess_data(text):\n","    sentence = nlp(text)\n","    cleaned_data = []\n","    for words in sentence:\n","        if words.text.lower() not in STOP_WORDS and words.text.isalpha():\n","            cleaned_data.append(words.lemma_.lower())\n","    return ' '.join(cleaned_data)"],"metadata":{"id":"H2ndtuhsAXgg","executionInfo":{"status":"ok","timestamp":1726605286875,"user_tz":-180,"elapsed":104,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# sentences1=[preprocess_data(text) for text in sentences]"],"metadata":{"id":"Ghq3qdQNAs0U","executionInfo":{"status":"ok","timestamp":1726605286876,"user_tz":-180,"elapsed":97,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# sentences[0]"],"metadata":{"id":"D7lU0CL-vmLu","executionInfo":{"status":"ok","timestamp":1726605286876,"user_tz":-180,"elapsed":96,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# output_file = '/content/drive/MyDrive/AI_RAG/Vector Database/AI_data.json'\n","\n","# with open(output_file, 'w', encoding='utf-8') as f:\n","#     json.dump(sentences1, f, ensure_ascii=False, indent=4)\n","\n","# print(f\"Preprocessed data successfully saved to {output_file}\")"],"metadata":{"id":"XLUKZk5CuCR8","executionInfo":{"status":"ok","timestamp":1726605286876,"user_tz":-180,"elapsed":89,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["json_file = '/content/drive/MyDrive/AI_RAG/Vector Database/AI_data.json'\n","\n","with open(json_file, 'r', encoding='utf-8') as f:\n","    sentences1 = json.load(f)"],"metadata":{"id":"Z9OtlLkFgahf","executionInfo":{"status":"ok","timestamp":1726605286877,"user_tz":-180,"elapsed":89,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# sentences1[:2]"],"metadata":{"id":"HCd3TCutD75I","executionInfo":{"status":"ok","timestamp":1726605286884,"user_tz":-180,"elapsed":78,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["EMBED_MODEL = \"all-MiniLM-L6-v2\"\n","\n","# client = chromadb.Client()\n","client = chromadb.PersistentClient(path=\"/content/drive/MyDrive/AI_RAG/Vector Database/vectordata1\")\n","\n"],"metadata":{"id":"imZiRf997zA0","executionInfo":{"status":"ok","timestamp":1726605286884,"user_tz":-180,"elapsed":77,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\n","collection1 = client.get_or_create_collection(\n","                                                name='AI_data',\n","                                                embedding_function=embedding_func,\n","                                                metadata={\"hnsw:space\": \"cosine\"},\n","                                            )\n","collection2 = client.get_or_create_collection(\n","                                                name='Machine-learning_data',\n","                                                embedding_function=embedding_func,\n","                                                metadata={\"hnsw:space\": \"cosine\"},\n","                                            )"],"metadata":{"id":"tBGSEM26HM2m","executionInfo":{"status":"ok","timestamp":1726605286891,"user_tz":-180,"elapsed":82,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["type(sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzDiExlVRtLG","executionInfo":{"status":"ok","timestamp":1726605286891,"user_tz":-180,"elapsed":75,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}},"outputId":"aaf37744-1606-40f4-860c-a6db5b1e725e"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["text_splitter1 = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n","\n","all_splits1 = []\n","\n","for sentence in texts1:\n","    splits = text_splitter1.split_text(sentence)\n","    all_splits1.extend(splits)\n"],"metadata":{"id":"MduZx4IzQ9_l","executionInfo":{"status":"ok","timestamp":1726605288152,"user_tz":-180,"elapsed":1312,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n","\n","all_splits2 = []\n","\n","for sentence in texts2:\n","    splits = text_splitter1.split_text(sentence)\n","    all_splits2.extend(splits)"],"metadata":{"id":"dAwqqCLigY1I","executionInfo":{"status":"ok","timestamp":1726605288153,"user_tz":-180,"elapsed":37,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["all_splits1[0], all_splits2[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvqSPB75Sl5_","executionInfo":{"status":"ok","timestamp":1726605288153,"user_tz":-180,"elapsed":36,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}},"outputId":"d6ce00ee-6c32-4f5e-eb4b-c32468c030bc"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('1\\nINTRODUCTION\\nIn which we try to explain why we consider artiﬁcial intelligence to be a subject\\nmost worthy of study, and in which we try to decide what exactly it is, this being a\\ngood thing to decide before embarking.\\nWe call ourselves Homo sapiens—man the wise—because our intelligence is so important\\nINTELLIGENCE\\nto us. For thousands of years, we have tried to understand how we think; that is, how a mere\\nhandful of matter can perceive, understand, predict, and manipulate a world far larger and\\nmore complicated than itself. The ﬁeld of artiﬁcial intelligence, or AI, goes further still: it\\nARTIFICIAL\\nINTELLIGENCE\\nattempts not just to understand but also to build intelligent entities.\\nAI is one of the newest ﬁelds in science and engineering. Work started in earnest soon\\nafter World War II, and the name itself was coined in 1956. Along with molecular biology,\\nAI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other disciplines.',\n"," 'Chapter 1. The Machine\\nLearning Landscape\\nWhen most people hear “Machine Learning,” they picture a robot: a\\ndependable butler or a deadly Terminator, depending on who you ask. But\\nMachine Learning is not just a futuristic fantasy; it’s already here. In fact,\\nit has been around for decades in some specialized applications, such as\\nOptical Character Recognition (OCR). But the first ML application that\\nreally became mainstream, improving the lives of hundreds of millions of\\npeople, took over the world back in the 1990s: the spam filter. It’s not\\nexactly a self-aware Skynet, but it does technically qualify as Machine\\nLearning (it has actually learned so well that you seldom need to flag an\\nemail as spam anymore). It was followed by hundreds of ML applications\\nthat now quietly power hundreds of products and features that you use\\nregularly, from better recommendations to voice search.\\nWhere does Machine Learning start and where does it end? What exactly')"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# ids1=[str(x) for x in range(len(sentences1))]\n","ids1=[f\"id_{i}\" for i in range(len(all_splits1))]\n","ids2=[f\"id_{i}\" for i in range(len(all_splits2))]"],"metadata":{"id":"P9p7M2gEOI1P","executionInfo":{"status":"ok","timestamp":1726605288153,"user_tz":-180,"elapsed":27,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["collection1.upsert(\n","                documents=all_splits1,\n","                ids=ids1\n","                        )\n","collection2.upsert(\n","                documents=all_splits2,\n","                ids=ids2\n","                        )"],"metadata":{"id":"JOS5tcdsO4eO","executionInfo":{"status":"ok","timestamp":1726606144752,"user_tz":-180,"elapsed":856620,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["query='What is Artificial Intelligence?'"],"metadata":{"id":"Qh5946pQbfGI","executionInfo":{"status":"ok","timestamp":1726606144753,"user_tz":-180,"elapsed":34,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# query function\n","def chroma_query(query):\n","    #query = query.lower()\n","    #query = preprocess_data(query)\n","    result1 = collection1.query(\n","                                query_texts=query,\n","                                n_results=1,\n","                                include=[\"documents\", \"distances\"]\n","                            )\n","    result2 = collection2.query(\n","                                query_texts=query,\n","                                n_results=1,\n","                                include=[\"documents\", \"distances\"]\n","                            )\n","\n","    return result1, result2"],"metadata":{"id":"Xapggdkgg8-B","executionInfo":{"status":"ok","timestamp":1726606305054,"user_tz":-180,"elapsed":487,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["result=chroma_query(query)\n","result"],"metadata":{"id":"XRLnP1wQhbUD","executionInfo":{"status":"ok","timestamp":1726606307634,"user_tz":-180,"elapsed":666,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6bf15408-db43-49a2-910f-b4c32dd50d86"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'ids': [['id_0']],\n","  'distances': [[0.3643718957901001]],\n","  'metadatas': None,\n","  'embeddings': None,\n","  'documents': [['1\\nINTRODUCTION\\nIn which we try to explain why we consider artiﬁcial intelligence to be a subject\\nmost worthy of study, and in which we try to decide what exactly it is, this being a\\ngood thing to decide before embarking.\\nWe call ourselves Homo sapiens—man the wise—because our intelligence is so important\\nINTELLIGENCE\\nto us. For thousands of years, we have tried to understand how we think; that is, how a mere\\nhandful of matter can perceive, understand, predict, and manipulate a world far larger and\\nmore complicated than itself. The ﬁeld of artiﬁcial intelligence, or AI, goes further still: it\\nARTIFICIAL\\nINTELLIGENCE\\nattempts not just to understand but also to build intelligent entities.\\nAI is one of the newest ﬁelds in science and engineering. Work started in earnest soon\\nafter World War II, and the name itself was coined in 1956. Along with molecular biology,\\nAI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other disciplines.']],\n","  'uris': None,\n","  'data': None,\n","  'included': ['documents', 'distances']},\n"," {'ids': [['id_3278']],\n","  'distances': [[0.5242549180984497]],\n","  'metadatas': None,\n","  'embeddings': None,\n","  'documents': [['randomly composed of parts of its parents’ genomes.\\n8  One interesting example of a genetic algorithm used for Reinforcement Learning is the\\nNeuroEvolution of Augmenting Topologies (NEAT) algorithm.\\n9  This is called Gradient Ascent. It’s just like Gradient Descent but in the opposite direction:\\nmaximizing instead of minimizing.\\n10  OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal']],\n","  'uris': None,\n","  'data': None,\n","  'included': ['documents', 'distances']})"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":[],"metadata":{"id":"WP6Paggok1c8","executionInfo":{"status":"ok","timestamp":1726606144754,"user_tz":-180,"elapsed":27,"user":{"displayName":"Risper Ndirangu","userId":"07072360610484586650"}}},"execution_count":56,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}